# text =  Они отличаются хорошей сбалансированностью: достаточно высокий уровень чувствительности одновременно с хорошим соотношением сигнал/шум и уровнем AOP (Acoustic Overload Point — это такой аналог максимального звукового давления для цифровых микрофонов).
Они O
отличаются O
хорошей O
сбалансированностью O
: O
достаточно O
высокий O
уровень O
чувствительности O
одновременно O
с O
хорошим O
соотношением O
сигнал O
/ O
шум O
и O
уровнем O
AOP B-TERM
( O
Acoustic B-TERM
Overload I-TERM
Point I-TERM
— O
это O
такой O
аналог O
максимального O
звукового O
давления O
для O
цифровых O
микрофонов O
) O
. O

# text =  Пожалуй, самое важное: мы отказались от моделей на базе DNN-HMM и перешли на архитектуру e2e-распознавания с использованием тяжёлых нейросетей-трансформеров.
Пожалуй O
, O
самое O
важное O
: O
мы O
отказались O
от O
моделей O
на O
базе O
DNN B-TERM
- I-TERM
HMM I-TERM
и O
перешли O
на O
архитектуру O
e2e B-TERM
- O
распознавания O
с O
использованием O
тяжёлых O
нейросетей O
- O
трансформеров O
. O

# text =  В обоих случаях они от Texas Instruments, но в Станции Макс используется более свежая и мощная модель TAS5825M.
В O
обоих O
случаях O
они O
от O
Texas B-TERM
Instruments I-TERM
, O
но O
в O
Станции B-TERM
Макс I-TERM
используется O
более O
свежая O
и O
мощная O
модель O
TAS5825M B-TERM
. O

# text =  В конкурсе приняли участие представители ведущих команд рынка компьютерной обработки текстов, в том числе «Антиплагиат», «Наносемантика», DeepPavlov.
В O
конкурсе O
приняли O
участие O
представители O
ведущих O
команд O
рынка O
компьютерной O
обработки O
текстов O
, O
в O
том O
числе O
« O
Антиплагиат B-TERM
» O
, O
« O
Наносемантика B-TERM
» O
, O
DeepPavlov B-TERM
. O

# text =  Руководитель проекта FirstTry Артём Щеголев пришел на конкурс в команде со своей супругой.
Руководитель O
проекта O
FirstTry B-TERM
Артём B-TERM
Щеголев I-TERM
пришел O
на O
конкурс O
в O
команде O
со O
своей O
супругой O
. O

# text =  TextIT API включает в себя функции проверки орфографии и исправления ошибок, формирования текстовой формы числительных (например, преобразовать “102 рубль” в “сто два рубля”), подсказки следующего слова по ранее введенному тексту, постановки слова в нужную словоформу (число, род, падеж, лицо и время) и другие полезные функции обработки и формирования текста.
TextIT B-TERM
API I-TERM
включает O
в O
себя O
функции O
проверки B-TERM
орфографии I-TERM
и O
исправления B-TERM
ошибок I-TERM
, O
формирования B-TERM
текстовой I-TERM
формы I-TERM
числительных I-TERM
( O
например O
, O
преобразовать O
“ O
102 O
рубль O
” O
в O
“ O
сто O
два O
рубля O
” O
) O
, O
подсказки B-TERM
следующего I-TERM
слова I-TERM
по O
ранее O
введенному O
тексту O
, O
постановки B-TERM
слова I-TERM
в I-TERM
нужную I-TERM
словоформу I-TERM
( O
число O
, O
род O
, O
падеж O
, O
лицо O
и O
время O
) O
и O
другие O
полезные O
функции O
обработки O
и O
формирования O
текста O
. 

# text = В данной статье я бы хотел познакомить читателей с одним из проектов Apache Software Foundation сообщества — NlpCraft.
В O
данной O
статье O
я O
бы O
хотел O
познакомить O
читателей O
с O
одним O
из O
проектов O
Apache B-TERM
Software I-TERM
Foundation I-TERM
сообщества O
— O
NlpCraft B-TERM
. O

# text = В этой статье я планирую представить читателям один из проектов сообщества Apache Software Foundation — NlpCraft.
В O
этой O
статье O
я O
планирую O
представить O
читателям O
один O
из O
проектов O
сообщества O
Apache B-TERM
Software I-TERM
Foundation I-TERM
— O
NlpCraft B-TERM
. O

# text =  NlpCraft — библиотека с открытым исходным кодом, предназначенная для интеграции языкового интерфейса в пользовательские приложения.
NlpCraft B-TERM
— O
библиотека O
с O
открытым O
исходным O
кодом O
, O
предназначенная O
для O
интеграции B-TERM
языкового I-TERM
интерфейса I-TERM
в O
пользовательские O
приложения O
. O

# text =  Подход Model-as-a-Code, позволяющий создавать и редактировать модели с помощью привычных разработчикам инструментов.
Подход O
Model B-TERM
- I-TERM
as I-TERM
- I-TERM
a I-TERM
- I-TERM
Code I-TERM
, O
позволяющий O
создавать O
и O
редактировать O
модели O
с O
помощью O
привычных O
разработчикам O
инструментов O
. O

# text =  Интеграция со множеством провайдеров NER компонентов (Apache OpenNlp, Stanford NLP, Google Natural Language API, Spacy)
Интеграция O
со O
множеством O
провайдеров O
NER B-TERM
компонентов O
( O
Apache B-TERM
OpenNlp I-TERM
, O
Stanford B-TERM
NLP I-TERM
, O
Google B-TERM
Natural I-TERM
Language I-TERM
API I-TERM
, O
Spacy B-TERM
) O

# text = Named Entity — именованная сущность.
Named B-TERM
Entity I-TERM
— O
именованная B-TERM
сущность I-TERM
. O

# text =  В большинстве случаев процесс конфигурации сводится к созданию и поддержке простого Json или Yaml файла.
В O
большинстве O
случаев O
процесс O
конфигурации O
сводится O
к O
созданию O
и O
поддержке O
простого O
Json B-TERM
или O
Yaml B-TERM
файла O
. O

# text =  Ближайшие и наиболее известные “аналоги“ Amazon Alexa и Google DialogFlow имеют целый ряд существенных отличий от данной системы.
Ближайшие O
и O
наиболее O
известные O
“ O
аналоги O
“ O
Amazon B-TERM
Alexa I-TERM
и O
Google B-TERM
DialogFlow I-TERM
имеют O
целый O
ряд O
существенных O
отличий O
от O
данной O
системы O
. O

# text = Онлайн-энциклопедия Wikipedia получила новый инструмент — сервис с элементами ИИ, который поможет автоматически определять некорректные правки материалов ресурса.
Онлайн O
- O
энциклопедия O
Wikipedia B-TERM
получила O
новый O
инструмент O
— O
сервис O
с O
элементами O
ИИ O
, O
который O
поможет O
автоматически O
определять B-TERM
некорректные I-TERM
правки I-TERM
материалов I-TERM
ресурса O
. O

# text =  Сервис ORES (Objective Revision Evaluation Service) будет проверять все правки на наличие спама или троллинга.
Сервис O
ORES B-TERM
( O
Objective B-TERM
Revision I-TERM
Evaluation I-TERM
Service I-TERM
) O
будет O
проверять O
все O
правки O
на O
наличие O
спама O
или O
троллинга O
. O

# text =  Сервис ORES (Objective Revision Evaluation Service) будет осуществлять проверку всех изменений на предмет наличия спама или троллинга.
Сервис O
ORES B-TERM
( O
Objective B-TERM
Revision I-TERM
Evaluation I-TERM
Service I-TERM
) O
будет O
осуществлять O
проверку O
всех O
изменений O
на O
предмет O
наличия O
спама O
или O
троллинга O
. O

# text =  Создателем ORES является Wikimedia Foundation.
Создателем O
ORES B-TERM
является O
Wikimedia B-TERM
Foundation I-TERM
. O

# text = ORES (Objective Revision Evaluation Service) был разработан Wikimedia Foundation.
ORES B-TERM
( O
Objective B-TERM
Revision I-TERM
Evaluation I-TERM
Service I-TERM
) O
был O
разработан B-TERM
Wikimedia B-TERM
Foundation I-TERM
. O

# text =  Вероятность того, что текст нормальный, составляет 0,0837.
Вероятность O
того O
, O
что O
текст O
нормальный O
, O
составляет O
0,0837 B-TERM
. O

# text =  Вероятность умышленной порчи текста — 0,9163.
Вероятность O
умышленной O
порчи O
текста O
— O
0,9163 B-TERM
. O

# text =  Точность распознавания естественного языка сейчас у лидеров когнитивных систем (IBM Watson, Google, ABBYY, Microsoft, Наносемантика) позволяет в общем понять смысл и ответить на письменный вопрос при заранее определенной предметной базы знаний, но разговор даже с 90% точностью распознавания фраз на самом деле очень утомителен.
Точность O
распознавания O
естественного O
языка O
сейчас O
у O
лидеров O
когнитивных O
систем O
( O
IBM B-TERM
Watson I-TERM
, O
Google B-TERM
, O
ABBYY B-TERM
, O
Microsoft B-TERM
, O
Наносемантика B-TERM
) O
позволяет O
в O
общем O
понять O
смысл O
и O
ответить O
на O
письменный O
вопрос O
при O
заранее O
определенной O
предметной O
базы O
знаний O
, O
но O
разговор O
даже O
с O
90 B-TERM
% I-TERM
точностью B-TERM
распознавания O
фраз O
на O
самом O
деле O
очень O
утомителен O
. O

# text =  Кластерный анализ корпуса текстов
Кластерный B-TERM
анализ I-TERM
корпуса O
текстов O

# text =  В качестве тестовых данных был взят фрагмент новостного датасета от РИА, из которого в обработке участвовали только заголовки новостей.
В O
качестве O
тестовых O
данных O
был O
взят O
фрагмент O
новостного O
датасета B-TERM
от I-TERM
РИА I-TERM
, O
из O
которого O
в O
обработке O
участвовали O
только O
заголовки O
новостей O
. O

# text =  Для векторизации текста использовалась модель LaBSE от @cointegrated.
Для O
векторизации O
текста O
использовалась O
модель O
LaBSE B-TERM
от O
@cointegrated B-TERM
. O

# text =  Я использовал для этого модель ruT5 за авторством @cointegrated.
Я O
использовал O
для O
этого O
модель O
ruT5 B-TERM
за O
авторством O
@cointegrated B-TERM
. O

# text =  После изучения мы поняли, что Дана (так мы называем нашего чат-бота) — это кнопочный и сценарный чат-бот.
После O
изучения O
мы O
поняли O
, O
что O
Дана B-TERM
( O
так O
мы O
называем O
нашего O
чат O
- O
бота O
) O
— O
это O
кнопочный O
и O
сценарный O
чат O
- O
бот O
. O

# text =  Среднее количество слов в запросе — четыре, поэтому мы собрали n-grams (словосочетания из 3-5 слов) и решили кластеризовать их разными методами типа тематического моделирования – agglomerative clustering поверх sentence embedding.
Среднее O
количество O
слов O
в O
запросе O
— O
четыре O
, O
поэтому O
мы O
собрали O
n B-TERM
- I-TERM
grams I-TERM
( O
словосочетания O
из O
3 O
- O
5 O
слов O
) O
и O
решили O
кластеризовать O
их O
разными O
методами O
типа O
тематического O
моделирования O
– O
agglomerative B-TERM
clustering I-TERM
поверх O
sentence B-TERM
embedding I-TERM
. O

# text =  Решили использовать стандартные метрики: precision, recall, f1 score, accuracy.
Решили O
использовать O
стандартные O
метрики O
: O
precision B-TERM
, O
recall B-TERM
, O
f1 B-TERM
score I-TERM
, O
accuracy B-TERM
. O

# text =  На всех классах accuracy = 0.65.
На O
всех O
классах O
accuracy B-TERM
= O
0.65 B-TERM
. O

# text =  Рассчитаем macro f1 score (0.72 + 0.28 + 0.66 + 0.66)/4 ~ 0.6, weighted f1 score (0.720.85+0.280.05+0.660.05+0.660.05) ~ 0.69.
Рассчитаем O
macro B-TERM
f1 I-TERM
score I-TERM
( O
0.72 O
+ O
0.28 O
+ O
0.66 O
+ O
0.66)/4 O
~ O
0.6 B-TERM
, O
weighted B-TERM
f1 I-TERM
score I-TERM
( O
0.720.85 O
+ O
0.280.05 O
+ O
0.660.05 O
+ O
0.660.05 O
) O
~ O
0.69 B-TERM
. O

# text =  Только на одном классе accuracy = 0.85.
Только O
на O
одном O
классе O
accuracy B-TERM
= O
0.85 B-TERM
. O

# text =  Рассчитаем macro f1 score (0.92 + 0 + 0 + 0)/4 ~ 0.23, weighted f1 score (0.920.85+00.05+00.05+00.05) ~ 0.78.
Рассчитаем O
macro B-TERM
f1 I-TERM
score I-TERM
( O
0.92 O
+ O
0 O
+ O
0 O
+ O
0)/4 O
~ O
0.23 B-TERM
, O
weighted B-TERM
f1 I-TERM
score I-TERM
( O
0.920.85 O
+ O
00.05 O
+ O
00.05 O
+ O
00.05 O
) O
~ O
0.78 B-TERM
. O


# text =  TCR (Task completed rate) — процент диалогов, в которых частично или полностью решили проблему абонента.
TCR B-TERM
( O
Task B-TERM
completed I-TERM
rate I-TERM
) O
— O
процент O
диалогов O
, O
в O
которых O
частично O
или O
полностью O
решили O
проблему O
абонента O
. O

# text =  CSI (Customer Satisfaction Index) — средняя оценка, которую поставили клиенты боту.
CSI B-TERM
( O
Customer B-TERM
Satisfaction I-TERM
Index I-TERM
) I-TERM
— O
средняя O
оценка O
, O
которую O
поставили O
клиенты O
боту O
. O

# text =  AR (Automation rate) — процент диалогов, в которых клиент не перешел на оператора.
AR B-TERM
( O
Automation B-TERM
rate I-TERM
) O
— O
процент O
диалогов O
, O
в O
которых O
клиент O
не O
перешел O
на O
оператора O
. O

# text =  Мы попробовали разные предобученные модели с ресурса Hugging Face.
Мы O
попробовали O
разные O
предобученные O
модели B-TERM
с O
ресурса O
Hugging B-TERM
Face I-TERM
. O

# text =  Лучший результат показала модель DeepPavlov/rubert-base-cased.
Лучший O
результат O
показала O
модель O
DeepPavlov B-TERM
/ I-TERM
rubert I-TERM
- I-TERM
base I-TERM
- I-TERM
cased I-TERM
. O

# text =  Методы, реализованные с помощью библиотеки nlpaugMask insert.
Методы O
, O
реализованные O
с O
помощью O
библиотеки O
nlpaugMask B-TERM
insert I-TERM
. O

# text =  Популярный способ аугментации через NMT-модели.
Популярный O
способ O
аугментации O
через O
NMT B-TERM
- I-TERM
модели I-TERM
. O

# text =  В результате переразметки и разделение моделей качество получилось следующее:Chain-model, sentiment, spamТак как в нашей задаче пространство всех интетов неопределенное (мы не знаем, сколько их), нужно было отделять имеющийся список интентов в чат-боте и остальную лексику, в которой могут содержаться и остальные интенты.
В O
результате O
переразметки O
и O
разделение O
моделей O
качество O
получилось O
следующее O
: O
Chain B-TERM
- I-TERM
model I-TERM
, O
sentiment B-TERM
, O
spamТак B-TERM
как O
в O
нашей O
задаче O
пространство O
всех O
интетов O
неопределенное O
( O
мы O
не O
знаем O
, O
сколько O
их O
) O
, O
нужно O
было O
отделять O
имеющийся O
список O
интентов O
в O
чат O
- O
боте O
и O
остальную O
лексику O
, O
в O
которой O
могут O
содержаться O
и O
остальные O
интенты O
. O

# text = Новая версия GPT-3, InstructGPT, лучше выполняет инструкции и выдает меньше оскорбительных выражений, дезинформации и ошибок в целом.
Новая O
версия O
GPT-3 B-TERM
, O
InstructGPT B-TERM
, O
лучше O
выполняет O
инструкции O
и O
выдает O
меньше O
оскорбительных O
выражений O
, O
дезинформации O
и O
ошибок O
в O
целом O
. O

# text =  В ходе тестирования модель китайских ученых показала улучшение на 2,74% по шкале F1 (оценка классификатора) сравнению с HFM (Hierarchical Fusion Model), представленной в прошлом году: новая нейросеть достигла 86% точности по сравнению с 83% у HFM.
В O
ходе O
тестирования O
модель O
китайских O
ученых O
показала O
улучшение O
на O
2,74 O
% O
по O
шкале O
F1 B-TERM
( O
оценка O
классификатора O
) O
сравнению O
с O
HFM B-TERM
( O
Hierarchical B-TERM
Fusion I-TERM
Model I-TERM
) O
, O
представленной O
в O
прошлом O
году O
: O
новая O
нейросеть O
достигла O
86 B-TERM
% I-TERM
точности B-TERM
по O
сравнению O
с O
83 B-TERM
% I-TERM
у O
HFM B-TERM
. O

# text =  Тем не менее, в Facebook признали, что алгоритмы пока не готовы к широкому развертыванию — точность их работы составляет около 65-70%.
Тем O
не O
менее O
, O
в O
Facebook B-TERM
признали O
, O
что O
алгоритмы O
пока O
не O
готовы O
к O
широкому O
развертыванию O
— O
точность B-TERM
их O
работы O
составляет O
около O
65 B-TERM
- I-TERM
70 I-TERM
% I-TERM
. O

# text =  Несмотря на все свои недостатки, библиотека (неожиданно) оказалась довольно востребованной — например, нагуглил, что pymorphy был слегка использован при разработке системы Speech-to-Text для русского языка в рамках французского проекта Quaero, и рекомендуется в качестве учебного материала в некоторых ВУЗах.
Несмотря O
на O
все O
свои O
недостатки O
, O
библиотека B-TERM
( O
неожиданно O
) O
оказалась O
довольно O
востребованной O
— O
например O
, O
нагуглил O
, O
что O
pymorphy B-TERM
был O
слегка O
использован O
при O
разработке O
системы O
Speech B-TERM
- I-TERM
to I-TERM
- I-TERM
Text I-TERM
для O
русского B-TERM
языка I-TERM
в O
рамках O
французского O
проекта O
Quaero B-TERM
, O
и O
рекомендуется O
в O
качестве O
учебного O
материала O
в O
некоторых O
ВУЗах O
. O

# text =  Решающим толчком к написанию pymorphy2 послужил проект OpenCorpora — ребята оттуда, кроме всего прочего (а там много «всего прочего»), взяли словарь из aot.ru, полностью переделали его структуру и занялись пополнением и прочими улучшениями.
Решающим O
толчком O
к O
написанию O
pymorphy2 B-TERM
послужил O
проект O
OpenCorpora B-TERM
— O
ребята O
оттуда O
, O
кроме O
всего O
прочего O
( O
а O
там O
много O
« O
всего O
прочего O
» O
) O
, O
взяли O
словарь O
из O
aot.ru B-TERM
, O
полностью O
переделали O
его O
структуру O
и O
занялись O
пополнением O
и O
прочими O
улучшениями O
. O

# text =  Что меня тут смутило: а) в статье ипользовалась C++ библиотека OpenFST (вроде самый популярный способ реализации конечных автоматов), но заставлять пользователей ставить ее вручную — не вариант; б) даже с использованием C++ библиотеки результаты, судя по статье, были достаточно скромные (2 тыс слов/сек против 100+ тыс слов/сек у mystem или lemmatizer); понятное дело, что эту цифру можно было бы, скорее всего, значительно улучшить (да и lightcaster пишет, что ничего не оптимизировал) — но все же; в) это один из тех подходов, который (по моему мнению) повышает порог вхождения — я считаю, что это скорее минус.
Что O
меня O
тут O
смутило O
: O
а O
) O
в O
статье O
ипользовалась O
C++ B-TERM
библиотека O
OpenFST B-TERM
( O
вроде O
самый O
популярный O
способ O
реализации O
конечных O
автоматов O
) O
, O
но O
заставлять O
пользователей O
ставить O
ее O
вручную O
— O
не O
вариант O
; O
б O
) O
даже O
с O
использованием O
C++ B-TERM
библиотеки O
результаты O
, O
судя O
по O
статье O
, O
были O
достаточно O
скромные O
( O
2 O
тыс O
слов O
/ O
сек O
против O
100 O
+ O
тыс O
слов O
/ O
сек O
у O
mystem O
или O
lemmatizer O
) O
; O
понятное O
дело O
, O
что O
эту O
цифру O
можно O
было O
бы O
, O
скорее O
всего O
, O
значительно O
улучшить O
( O
да O
и O
lightcaster O
пишет O
, O
что O
ничего O
не O
оптимизировал O
) O
— O
но O
все O
же O
; O
в O
) O
это O
один O
из O
тех O
подходов O
, O
который O
( O
по O
моему O
мнению O
) O
повышает O
порог O
вхождения O
— O
я O
считаю O
, O
что O
это O
скорее O
минус O
. O

# text =  В итоге получалось, что мне нужно было бы: разобраться, как оптимизировать код и почему даже с C++ библиотекой получается так медленно; написать более простую в установке обертку для OpenFST (или использовать другую реализацию FST — например, сделать свою) + сделать реализацию небольшой части OpenFST (или просто реализацию FST) на Python (чтоб pymorphy можно было использовать без компилятора), ну и формулировать все алгоритмы в терминах конечных автоматов.
В O
итоге O
получалось O
, O
что O
мне O
нужно O
было O
бы O
: O
разобраться O
, O
как O
оптимизировать O
код O
и O
почему O
даже O
с O
C++ B-TERM
библиотекой O
получается O
так O
медленно O
; O
написать O
более O
простую O
в O
установке O
обертку O
для O
OpenFST B-TERM
( O
или O
использовать O
другую O
реализацию O
FST B-TERM
— O
например O
, O
сделать O
свою O
) O
+ O
сделать O
реализацию O
небольшой O
части O
OpenFST B-TERM
( O
или O
просто O
реализацию O
FST B-TERM
) O
на O
Python B-TERM
( O
чтоб O
pymorphy O
можно O
было O
использовать O
без O
компилятора O
) O
, O
ну O
и O
формулировать O
все O
алгоритмы O
в O
терминах O
конечных O
автоматов O
. O

# text =  Сперва мне приглянулась библиотека libdatrie, про обертку для нее писал тут: habrahabr.ru/post/147963.
Сперва O
мне O
приглянулась O
библиотека O
libdatrie B-TERM
, O
про O
обертку O
для O
нее O
писал O
тут O
: O
habrahabr.ru/post/147963 O
. O

# text =  Но и этот второй вариант оставался узким местом, даже переписанный на Cython — что с ним делать, я не знал.
Но O
и O
этот O
второй O
вариант O
оставался O
узким O
местом O
, O
даже O
переписанный O
на O
Cython B-TERM
— O
что O
с O
ним O
делать O
, O
я O
не O
знал O
. O

# text =  Выбор пал на C++ библиотеку marisa-trie, которую написал гуру структур данных Susumu Yata.
Выбор O
пал O
на O
C++ B-TERM
библиотеку O
marisa B-TERM
- I-TERM
trie I-TERM
, O
которую O
написал O
гуру O
структур O
данных O
Susumu B-TERM
Yata I-TERM
. O

# text = Было решено использовать библиотеку marisa-trie на C++, созданную экспертом в области структур данных Сусуму Ята.
Было O
решено O
использовать O
библиотеку O
marisa B-TERM
- I-TERM
trie I-TERM
на O
C++ B-TERM
, O
созданную O
экспертом O
в O
области O
структур O
данных O
Сусуму B-TERM
Ята I-TERM
. O

# text =  С другой стороны, ускорить версию под CPython понятно как — переписать еще что-нибудь на Cython (к слову: делать я этого не планирую); с PyPy это не так очевидно.
С O
другой O
стороны O
, O
ускорить O
версию O
под O
CPython B-TERM
понятно O
как O
— O
переписать O
еще O
что O
- O
нибудь O
на O
Cython B-TERM
( O
к O
слову O
: O
делать O
я O
этого O
не O
планирую O
) O
; O
с O
PyPy B-TERM
это O
не O
так O
очевидно O
. O

# text =  Если не использовать ни PyPy, ни C++ реализацию DAWG, pymorphy2 все равно будет работать во много раз быстрее (по прикидкам — в пару десятков раз), чем pymorphy1 cо всеми включенными ускорениями — ну и разбирать лучше.
Если O
не O
использовать O
ни O
PyPy B-TERM
, O
ни O
C++ B-TERM
реализацию O
DAWG B-TERM
, O
pymorphy2 B-TERM
все O
равно O
будет O
работать O
во O
много O
раз O
быстрее O
( O
по O
прикидкам O
— O
в O
пару O
десятков O
раз O
) O
, O
чем O
pymorphy1 B-TERM
cо O
всеми O
включенными O
ускорениями O
— O
ну O
и O
разбирать O
лучше O
. O


# text =  Там сейчас есть фичи, которых нет в pymorphy2 (например, интеграция с django, согласование слов с цифрами и склонение фамилий), но в версии на битбакете я поломал обратную совместимость.
Там O
сейчас O
есть O
фичи O
, O
которых O
нет O
в O
pymorphy2 B-TERM
( O
например O
, O
интеграция O
с O
django B-TERM
, O
согласование O
слов O
с O
цифрами O
и O
склонение O
фамилий O
) O
, O
но O
в O
версии O
на O
битбакете O
я O
поломал O
обратную O
совместимость O
. O

# text =  К примеру, одна из наиболее известных систем такого типа Grammarly выступает против использования пассивного залога.
К O
примеру O
, O
одна O
из O
наиболее O
известных O
систем O
такого O
типа O
Grammarly B-TERM
выступает O
против O
использования O
пассивного O
залога O
. O

# text =  Например, сервис Textly.AI разрешает использовать свои плагины для Chrome и Firefox без создания аккаунта, а в их веб-приложении есть режим, похожий на секретный чат в браузере – после выхода из него, весь откорректированный алгоритмом контент удаляется.
Например O
, O
сервис O
Textly B-TERM
. I-TERM
AI I-TERM
разрешает O
использовать O
свои O
плагины O
для O
Chrome B-TERM
и O
Firefox B-TERM
без O
создания O
аккаунта O
, O
а O
в O
их O
веб O
- O
приложении O
есть O
режим O
, O
похожий O
на O
секретный O
чат O
в O
браузере O
– O
после O
выхода O
из O
него O
, O
весь O
откорректированный O
алгоритмом O
контент O
удаляется O
. O

# text =  Еще пример: сервис Ginger предлагает инструмент для перефразирования предложений – пользователь может написать совсем простое, а ему дадут чуть более сложный вариант (не совсем то, что нужно, но хоть что-то).
Еще O
пример O
: O
сервис O
Ginger B-TERM
предлагает O
инструмент O
для O
перефразирования O
предложений O
– O
пользователь O
может O
написать O
совсем O
простое O
, O
а O
ему O
дадут O
чуть O
более O
сложный O
вариант O
( O
не O
совсем O
то O
, O
что O
нужно O
, O
но O
хоть O
что O
- O
то O
) O
. O

# text =  Например, на NLP-progress публикуются последние достижения в области commonsense reasoning.
Например O
, O
на O
NLP B-TERM
- O
progress O
публикуются O
последние O
достижения O
в O
области O
commonsense B-TERM
reasoning I-TERM
. O

# text =  В этом посте мы расскажем, как мы создали датасет для задачи Common Sense Reasoning в одной из ее возможных формулировок, предложенной в статье event2mind, а также адаптировали английскую модель event2mind от AllenNLP для русского языка.
В O
этом O
посте O
мы O
расскажем O
, O
как O
мы O
создали O
датасет O
для O
задачи O
Common B-TERM
Sense I-TERM
Reasoning I-TERM
в O
одной O
из O
ее O
возможных O
формулировок O
, O
предложенной O
в O
статье O
event2mind O
, O
а O
также O
адаптировали O
английскую O
модель O
event2mind B-TERM
от O
AllenNLP B-TERM
для O
русского B-TERM
языка I-TERM
. O

# text =  Тексты из SynTagRus, который является частью Русского Национального корпуса и содержит художественные тексты вместе с новостями.
Тексты O
из O
SynTagRus B-TERM
, O
который O
является O
частью O
Русского B-TERM
Национального I-TERM
корпуса I-TERM
и O
содержит O
художественные O
тексты O
вместе O
с O
новостями O
. O

# text =  Для поиска подобных паттернов был использован синтаксический парсер UdPipe, с помощью которого мы выделяли в текстах паттерны вида глагол + зависимые слова в синтаксическом дереве, как например на рисунке 2, которые удовлетворяли одному из следующих правил:
Для O
поиска O
подобных O
паттернов O
был O
использован O
синтаксический O
парсер O
UdPipe B-TERM
, O
с O
помощью O
которого O
мы O
выделяли O
в O
текстах O
паттерны O
вида O
глагол O
+ O
зависимые O
слова O
в O
синтаксическом O
дереве O
, O
как O
например O
на O
рисунке O
2 O
, O
которые O
удовлетворяли O
одному O
из O
следующих O
правил O

# text =  Однако уже при 30000 примеров, loss и recall практически не отличаются от результатов на полном объеме данных.
Однако O
уже O
при O
30000 O
примеров O
, O
loss B-TERM
и O
recall B-TERM
практически O
не O
отличаются O
от O
результатов O
на O
полном O
объеме O
данных O
. O

# text =  Изначально английский корпус собран из нескольких источников: ROC Story training set, the GoogleSyntactic N-grams, the Spinn3r corpus и idioms.
Изначально O
английский O
корпус O
собран O
из O
нескольких O
источников O
: O
ROC B-TERM
Story I-TERM
training I-TERM
set I-TERM
, O
the B-TERM
GoogleSyntactic I-TERM
N I-TERM
- I-TERM
grams I-TERM
, O
the B-TERM
Spinn3r I-TERM
corpus I-TERM
и O
idioms B-TERM
. O

# text =  Поэтому мы взяли только примеры из ROC-story.
Поэтому O
мы O
взяли O
только O
примеры O
из O
ROC B-TERM
- I-TERM
Story I-TERM
. O

# text =  таблицу 2), у этого источника коэффициент согласованности аннотаторов (Cohen's kappa coefficient), равный 0.57.
таблицу O
2 O
) O
, O
у O
этого O
источника O
коэффициент B-TERM
согласованности I-TERM
аннотаторов I-TERM
( O
Cohen B-TERM
's I-TERM
kappa I-TERM
coefficient I-TERM
) O
, O
равный O
0.57 B-TERM
. O

# text =  При этом fasttext embeddings, обученные на ruscorpora показали себя лучше обученных на araneum.
При O
этом O
fasttext B-TERM
embeddings I-TERM
, O
обученные O
на O
ruscorpora B-TERM
показали O
себя O
лучше O
обученных O
на O
araneum B-TERM
. O

# text =  Сегодня мы расскажем, как помогли НПО Энергомаш создать корпоративную интеллектуальную информационно-поисковую систему (КИИПС) на базе ABBYY Intelligent Search – такую же удобную и быструю, как популярные поисковики.
Сегодня O
мы O
расскажем O
, O
как O
помогли O
НПО B-TERM
Энергомаш I-TERM
создать O
корпоративную B-TERM
интеллектуальную I-TERM
информационно I-TERM
- I-TERM
поисковую I-TERM
систему I-TERM
( O
КИИПС B-TERM
) O
на O
базе O
ABBYY B-TERM
Intelligent I-TERM
Search I-TERMI-App_system
– O
такую O
же O
удобную O
и O
быструю O
, O
как O
популярные O
поисковики O
. O

# text =  Сегодня мы расскажем о том, как мы содействовали НПО "Энергомаш" в разработке и внедрении их корпоративной интеллектуальной информационно-поисковой системы (КИИПС).
Сегодня O
мы O
расскажем O
о O
том O
, O
как O
мы O
содействовали O
НПО B-TERM
" I-TERM
Энергомаш I-TERM
" I-TERM
в O
разработке O
и O
внедрении O
их O
корпоративной B-TERM
интеллектуальной I-TERM
информационно I-TERM
- I-TERM
поисковой I-TERM
системы I-TERM
( O
КИИПС B-TERM
) O
. O

# text =  Энергомаш рассматривал несколько поисковых систем, но в итоге решил попробовать ABBYY Intelligent Search.
Энергомаш B-TERM
рассматривал O
несколько O
поисковых O
систем O
, O
но O
в O
итоге O
решил O
попробовать O
ABBYY B-TERM
Intelligent I-TERM
Search I-TERM
. O

# text =  Энергомаш подключил к поиску 7 корпоративных источников: систему электронного документооборота LanDocs, файловое хранилище, ИБД, систему поддержки жизненного цикла изделия TeamCenter, систему управления ресурсами Галактика ERP и AMM, информационную систему управления проектами.
Энергомаш B-TERM
подключил O
к O
поиску O
7 O
корпоративных O
источников O
: O
систему B-TERM
электронного I-TERM
документооборота I-TERM
LanDocs I-TERM
, O
файловое O
хранилище O
, O
ИБД B-TERM
, O
систему O
поддержки O
жизненного O
цикла O
изделия O
TeamCenter B-TERM
, O
систему O
управления O
ресурсами O
Галактика B-TERM
ERP I-TERM
и O
AMM B-TERM
, O
информационную O
систему O
управления O
проектами O
. O

# text =  Доступ в Систему корпоративного поиска организован через внутренний портал предприятия на главной странице.
Доступ O
в O
Систему B-TERM
корпоративного I-TERM
поиска I-TERM
организован O
через O
внутренний O
портал O
предприятия O
на O
главной O
странице O
. O

# text =  По данным Google Books Ngram Viewer — поискового онлайн-сервиса Google, который строит графики частоты упоминания языковых единиц на основе огромного количества печатных источников, популярность и интерес к NLP стремительно растет последние 20 лет.
По O
данным O
Google B-TERM
Books I-TERM
Ngram I-TERM
Viewer I-TERM
— O
поискового O
онлайн O
- O
сервиса O
Google B-TERM
, O
который O
строит O
графики O
частоты O
упоминания O
языковых O
единиц O
на O
основе O
огромного O
количества O
печатных O
источников O
, O
популярность O
и O
интерес O
к O
NLP B-TERM
стремительно O
растет O
последние O
20 O
лет O
. O

# text =  Heliograf способен генерировать новостные, финансовые и подобные им отчеты, и даже посты для социальных медиа.
Heliograf B-TERM
способен O
генерировать O
новостные O
, O
финансовые O
и O
подобные O
им O
отчеты O
, O
и O
даже O
посты O
для O
социальных O
медиа O
. O

# text =  Так же, как Wordsmith, эта система используется репортерами при подготовке тысяч корпоративных финансовых отчетов, помогая Bloomberg News в нелегкой конкурентной борьбе с агентством Reuters, а также с новыми участниками информационной гонки – продвинутыми хедж-фондами, которые также используют системы на базе ИИ для поставки свежих новостей и аналитики своим клиентам.
Так O
же O
, O
как O
Wordsmith B-TERM
, O
эта O
система O
используется O
репортерами O
при O
подготовке O
тысяч O
корпоративных O
финансовых O
отчетов O
, O
помогая O
Bloomberg B-TERM
News I-TERM
в O
нелегкой O
конкурентной O
борьбе O
с O
агентством O
Reuters B-TERM
, O
а O
также O
с O
новыми O
участниками O
информационной O
гонки O
– O
продвинутыми O
хедж O
- O
фондами O
, O
которые O
также O
используют O
системы O
на O
базе O
ИИ O
для O
поставки O
свежих O
новостей O
и O
аналитики O
своим O
клиентам O
. O

# text = Наконец, компания Forbes недавно сообщила, что тестирует собственную систему Bertie, которая помогает журналистам с написанием черновых вариантов и шаблонов статей.
Наконец O
, O
компания O
Forbes B-TERM
недавно O
сообщила O
, O
что O
тестирует O
собственную O
систему O
Bertie B-TERM
, O
которая O
помогает O
журналистам O
с O
написанием O
черновых O
вариантов O
и O
шаблонов O
статей O
. O

# text =  Так как например модель «wiki_ru», содержит в своем корпусе 1,88 млн слов в словаре, и 2 млн n-грамм токенов, (300 мерных) векторов.
Так O
как O
например O
модель O
« O
wiki_ru B-TERM
» O
, O
содержит O
в O
своем O
корпусе O
1,88 O
млн O
слов O
в O
словаре O
, O
и O
2 O
млн O
n O
- O
грамм O
токенов O
, O
( O
300 O
мерных O
) O
векторов O
. O

# text =  Мы в Badoo и Bumble стараемся оградить пользователей от неприятных ситуаций, поэтому внедрили инструмент Rude Message Detector.
Мы O
в O
Badoo B-TERM
и O
Bumble B-TERM
стараемся O
оградить O
пользователей O
от O
неприятных O
ситуаций O
, O
поэтому O
внедрили O
инструмент O
Rude B-TERM
Message I-TERM
Detector I-TERM
. O

# text =  Она очень похожа на SentencePiece, но выполняет чуть больше действий над некоторыми токенами
Она O
очень O
похожа O
на O
SentencePiece B-TERM
, O
но O
выполняет O
чуть O
больше O
действий O
над O
некоторыми O
токенами

# text = Нативная TensorFlow-реализация токенизатора требуется для XLM-RoBERTa.
Нативная O
TensorFlow B-TERM
- O
реализация O
токенизатора O
требуется O
для O
XLM B-TERM
- I-TERM
RoBERTa I-TERM
. O

# text =  Данные: 30 GB русского текста, в котором была Википедия, новости, часть корпуса Taiga и немного книг.
Данные O
: O
30 O
GB O
русского O
текста O
, O
в O
котором O
была O
Википедия B-TERM
, O
новости O
, O
часть O
корпуса O
Taiga B-TERM
и O
немного O
книг O
. O

# text =  Основным из них был SberSQUAD.
Основным O
из O
них O
был O
SberSQUAD B-TERM
. O

# text =  Позднее, на RussianSuperGLUE, она показала себя чуть лучше.
Позднее O
, O
на O
RussianSuperGLUE B-TERM
, O
она O
показала O
себя O
чуть O
лучше O
. O

# text =  В последствии эта модель легла в основу модели SBERT наших коллег из смежной команды SberDevices, которую они выложили в открытый доступ.
В O
последствии O
эта O
модель O
легла O
в O
основу O
модели O
SBERT B-TERM
наших O
коллег O
из O
смежной O
команды O
SberDevices B-TERM
, O
которую O
они O
выложили O
в O
открытый O
доступ O
. O

# text =  Детали: немного переделали код для обучения из библиотеки Transformers.
Детали O
: O
немного O
переделали O
код O
для O
обучения O
из O
библиотеки O
Transformers B-TERM
. O

# text =  Обучали на всём русском корпусе, что у нас был для ruGPT-3 (Википедия, книги, новости, русский Common Crawl и т.д.).
Обучали O
на O
всём O
русском O
корпусе O
, O
что O
у O
нас O
был O
для O
ruGPT-3 B-TERM
( O
Википедия B-TERM
, O
книги O
, O
новости O
, O
русский O
Common B-TERM
Crawl I-TERM
и O
т O
. O
д. O
) O
. O

# text =  На разных заданиях топовые метрики далеко не только у ruT5-large: лучшие single-model решения также есть у ruRoBERTa-large (задача TERRa и DaNetQA).
На O
разных O
заданиях O
топовые O
метрики O
далеко O
не O
только O
у O
ruT5-large B-TERM
: O
лучшие O
single O
- O
model O
решения O
также O
есть O
у O
ruRoBERTa B-TERM
- I-TERM
large I-TERM
( O
задача O
TERRa B-TERM
и O
DaNetQA B-TERM
) O
. O

# text =  А на некоторых заданиях лучших результатов удалось достичь на few-shot, т. е. без дообучения модели (задача RCB и RuCoS, YaLM от Яндекса).
А O
на O
некоторых O
заданиях O
лучших O
результатов O
удалось O
достичь O
на O
few B-TERM
- I-TERM
shot I-TERM
, O
т O
. O
е O
. O
без O
дообучения O
модели O
( O
задача O
RCB B-TERM
и O
RuCoS B-TERM
, O
YaLM B-TERM
от O
Яндекса B-TERM
) O
. O

# text =  Эти три модели из библиотеки Hugging Face — самые популярные на сегодняшний день.
Эти O
три O
модели O
из O
библиотеки O
Hugging B-TERM
Face I-TERM
— O
самые O
популярные O
на O
сегодняшний O
день O
. O

# text =  Есть стандартная задача извлечения именованных сущностей из текста (NER).
Есть O
стандартная O
задача O
извлечения O
именованных O
сущностей O
из O
текста O
( O
NER B-TERM
) O
. O

# text =  Задача старая и хорошо изученная, для английского языка существует масса коммерческих и открытых решений: Spacy, Stanford NER, OpenNLP, NLTK, MITIE, Google Natural Language API, ParallelDots, Aylien, Rosette, TextRazor.
Задача O
старая O
и O
хорошо O
изученная O
, O
для O
английского B-TERM
языка I-TERM
существует O
масса O
коммерческих O
и O
открытых O
решений O
: O
Spacy B-TERM
, O
Stanford B-TERM
NER I-TERM
, O
OpenNLP B-TERM
, O
NLTK B-TERM
, O
MITIE B-TERM
, O
Google B-TERM
Natural I-TERM
Language I-TERM
API I-TERM
, O
ParallelDots B-TERM
, O
Aylien B-TERM
, O
Rosette B-TERM
, O
TextRazor B-TERM
. O

# text =  Для русского тоже есть хорошие решения, но они в основном закрытые: DaData, Pullenti, Abbyy Infoextractor, Dictum, Eureka, Promt, RCO, AOT, Ahunter.
Для O
русского O
тоже O
есть O
хорошие O
решения O
, O
но O
они O
в O
основном O
закрытые O
: O
DaData B-TERM
, O
Pullenti B-TERM
, O
Abbyy B-TERM
Infoextractor I-TERM
, O
Dictum B-TERM
, O
Eureka B-TERM
, O
Promt B-TERM
, O
RCO B-TERM
, O
AOT B-TERM
, O
Ahunter B-TERM
. O

# text =  Из открытого мне известен только Томита-парсер и свежий Deepmipt NER.
Из O
открытого O
мне O
известен O
только O
Томита B-TERM
- I-TERM
парсер I-TERM
и O
свежий O
Deepmipt B-TERM
NER I-TERM
. O

# text =  Год назад Дима Веселов начал проект Natasha.
Год O
назад O
Дима B-TERM
Веселов I-TERM
начал O
проект O
Natasha B-TERM
. O

# text =  Natasha — это аналог Томита-парсера для Python (Yargy-парсер) плюс набор готовых правил для извлечения имён, адресов, дат, сумм денег и других сущностей.
Natasha B-TERM
— O
это O
аналог O
Томита B-TERM
- I-TERM
парсера I-TERM
для O
Python B-TERM
( O
Yargy B-TERM
- I-TERM
парсер I-TERM
) O
плюс O
набор O
готовых O
правил O
для O
извлечения O
имён O
, O
адресов O
, O
дат O
, O
сумм O
денег O
и O
других O
сущностей O
. O

# text =  У топовых решений F1-мера для имён была 0.9+.
У O
топовых O
решений O
F1-мера B-TERM
для O
имён O
была O
0.9 B-TERM
+ O
. O

# text =  У Natasha результат хуже — 0.78.
У O
Natasha B-TERM
результат O
хуже O
— O
0.78 B-TERM
. O

# text =  Yargy — сложная и интересная библиотека, в этой статье мы рассмотрим только простые примеры использования.
Yargy B-TERM
— O
сложная O
и O
интересная O
библиотека O
, O
в O
этой O
статье O
мы O
рассмотрим O
только O
простые O
примеры O
использования O
. O

# text =  В словаре Opencorpora, который использует pymorphy2, для имён ставится метка Name, для фамилий — метка Surn.
В O
словаре O
Opencorpora B-TERM
, O
который O
использует O
pymorphy2 B-TERM
, O
для O
имён O
ставится O
метка O
Name O
, O
для O
фамилий O
— O
метка O
Surn O
. O

# text =  Например, качество извлечения имён у Natasha очень далеко от SOTA.
Например O
, O
качество O
извлечения O
имён O
у O
Natasha B-TERM
очень O
далеко O
от O
SOTA B-TERM
. O

# text =  В начале 2018 года исследователи из OpenAI, университета Сан-Франциско, Алленовского института искусственного интеллекта и Вашингтонского университета одновременно вывели хитроумный способ приблизиться к этому.
В O
начале O
2018 B-TERM
года O
исследователи O
из O
OpenAI B-TERM
, O
университета O
Сан O
- O
Франциско O
, O
Алленовского B-TERM
института I-TERM
искусственного I-TERM
интеллекта I-TERM
и O
Вашингтонского B-TERM
университета I-TERM
одновременно O
вывели O
хитроумный O
способ O
приблизиться O
к O
этому O
. O

# text =  Набор данных назвали Гансом (Heuristic Analysis for Natural-Language-Inference Systems, HANS) [эвристический анализ систем, делающих заключения на основе естественного языка].
Набор O
данных O
назвали O
Гансом B-TERM
( O
Heuristic B-TERM
Analysis I-TERM
for I-TERM
Natural I-TERM
- I-TERM
Language I-TERM
- I-TERM
Inference I-TERM
Systems I-TERM
, O
HANS B-TERM
) O
[ O
эвристический O
анализ O
систем O
, O
делающих O
заключения O
на O
основе O
естественного O
языка O
] O
. O

# text =  В нашей задаче мы использовали набор данных Ганс (Heuristic Analysis for Natural-Language-Inference Systems, HANS). 
В O
нашей O
задаче O
мы O
использовали O
набор O
данных O
Ганс B-TERM
( O
Heuristic B-TERM
Analysis I-TERM
for I-TERM
Natural I-TERM
- I-TERM
Language I-TERM
- I-TERM
Inference I-TERM
Systems I-TERM
, O
HANS B-TERM
) O
. O

# text =  Рады представить вам PyCaret – библиотеку машинного обучения с открытым исходным кодом на Python для обучения и развертывания моделей с учителем и без учителя в low-code среде.
Рады O
представить O
вам O
PyCaret B-TERM
– O
библиотеку O
машинного O
обучения O
с O
открытым O
исходным O
кодом O
на O
Python B-TERM
для O
обучения O
и O
развертывания O
моделей O
с O
учителем O
и O
без O
учителя O
в O
low O
- O
code O
среде O
. O

# text =  PyCaret позволит вам пройти путь от подготовки данных до развертывания модели за несколько секунд в той notebook-среде, которую вы выберете.
PyCaret B-TERM
позволит O
вам O
пройти O
путь O
от O
подготовки O
данных O
до O
развертывания O
модели O
за O
несколько O
секунд O
в O
той O
notebook B-TERM
- O
среде O
, O
которую O
вы O
выберете O
. O

# text =  PyCaret – это, по сути, оболочка Python над несколькими библиотеками машинного обучения, такими как scikit-learn, XGBoost, Microsoft LightGBM, spaCy и многими другими.
PyCaret B-TERM
– O
это O
, O
по O
сути O
, O
оболочка O
Python B-TERM
над O
несколькими O
библиотеками O
машинного O
обучения O
, O
такими O
как O
scikit B-TERM
- I-TERM
learn I-TERM
, O
XGBoost B-TERM
, O
Microsoft B-TERM
LightGBM I-TERM
, O
spaCy O
и O
многими O
другими O
. O

# text =  Первый стабильный релиз PyCaret версии 1.0.0 можно установить с помощью pip.
Первый O
стабильный O
релиз O
PyCaret B-TERM
версии O
1.0.0 O
можно O
установить O
с O
помощью O
pip B-TERM
. O

# text =  Этот датасет доступен на GitHub-репозитории PyCaret.
Этот O
датасет O
доступен O
на O
GitHub B-TERM
- O
репозитории O
PyCaret B-TERM
. O

# text =  Для классификации: Accuracy, AUC, Recall, Precision, F1, Kappa. 
Для O
классификации B-TERM
: O
Accuracy B-TERM
, O
AUC B-TERM
, O
Recall B-TERM
, O
Precision B-TERM
, O
F1 B-TERM
, O
Kappa B-TERM
. O

# text =  Для регрессии: MAE, MSE, RMSE, R2, RMSLE, MAPE 
Для O
регрессии B-TERM
: O
MAE B-TERM
, O
MSE B-TERM
, O
RMSE B-TERM
, O
R2 B-TERM
, O
RMSLE B-TERM
, O
MAPE B-TERM
. O

# text =  Стэнфордская нейросеть определяет тональность текста с точностью 85%
Стэнфордская B-TERM
нейросеть I-TERM
определяет O
тональность O
текста O
с O
точностью B-TERM
85 B-TERM
% I-TERM

# text =  Нейросеть от Стэнфорда демонстрирует точность в 85% при определении тональности текста. 
Нейросеть B-TERM
от O
Стэнфорда B-TERM
демонстрирует O
точность B-TERM
в O
85 B-TERM
% I-TERM
при O
определении O
тональности O
текста O
. O

# text =  Для англоязычного nlp-сообщества задача поиска сложного слова в тексте называется так: CWI – complex word identification.
Для O
англоязычного O
nlp O
- O
сообщества O
задача O
поиска O
сложного O
слова O
в O
тексте O
называется O
так O
: O
CWI B-TERM
– O
complex B-TERM
word I-TERM
identification I-TERM
. O

# text =  Примеры обучения LDA часто демонстрируются на "образцовых" датасетах, например "20 newsgroups dataset", который есть в sklearn.
Примеры O
обучения O
LDA B-TERM
часто O
демонстрируются O
на O
" O
образцовых O
" O
датасетах O
, O
например O
" O
20 B-TERM
newsgroups I-TERM
dataset I-TERM
" O
, O
который O
есть O
в O
sklearn B-TERM
. O

# text =  Для стемминга использовался pymystem3.
Для O
стемминга O
использовался O
pymystem3 B-TERM
. O

# text = Объединенная команда специалистов Пенсильванского и Шеффилдского университетов создала слабую форму искусственного интеллекта, которая способна предсказывать решения Европейского суда по правам человека (European Court of Human Rights, ECtHR, ЕСПЧ) с точностью в 79%.
Объединенная O
команда O
специалистов O
Пенсильванского B-TERM
и O
Шеффилдского B-TERM
университетов I-TERM
создала O
слабую O
форму O
искусственного O
интеллекта O
, O
которая O
способна O
предсказывать O
решения O
Европейского O
суда O
по O
правам O
человека O
( O
European O
Court O
of O
Human O
Rights O
, O
ECtHR O
, O
ЕСПЧ O
) O
с O
точностью B-TERM
в O
79 B-TERM
% I-TERM
. O

# text =  Он демонстрирует точность 95%.
Он O
демонстрирует O
точность B-TERM
95 B-TERM
% I-TERM
. O

# text =  На данный момент подход дает прогноз с accuracy 0,64, что выше случайного предсказания.
На O
данный O
момент O
подход B-TERM
дает O
прогноз O
с O
accuracy B-TERM
0,64 B-TERM
, O
что O
выше O
случайного O
предсказания O
. O

# text =  Таким образом, хоть и удается достигнуть высокой точности, но результат не всегда стабилен и в моем случае колеблется в промежутке 75-80%.
Таким O
образом O
, O
хоть O
и O
удается O
достигнуть O
высокой O
точности B-TERM
, O
но O
результат O
не O
всегда O
стабилен O
и O
в O
моем O
случае O
колеблется O
в O
промежутке O
75 B-TERM
- I-TERM
80 I-TERM
% I-TERM
. O

# text =  Видно, что при перестановках качество падает, но это падение не критично и точность остается в диапазоне 69-80%.
Видно O
, O
что O
при O
перестановках O
качество O
падает O
, O
но O
это O
падение O
не O
критично O
и O
точность B-TERM
остается O
в O
диапазоне O
69 B-TERM
- I-TERM
80 I-TERM
% I-TERM
. O

# text =  ЗаключениеВ итоге, моделью RuBioRoBERTa в задаче RuMedDaNet мне удалось добиться качества 73.24% на закрытой тестовой части данных (хотя на dev метрика Accuracy вообще была 81.64%).
ЗаключениеВ O
итоге O
, O
моделью O
RuBioRoBERTa B-TERM
в O
задаче O
RuMedDaNet B-TERM
мне O
удалось O
добиться O
качества O
73.24 O
% O
на O
закрытой O
тестовой O
части O
данных O
( O
хотя O
на O
dev O
метрика O
Accuracy B-TERM
вообще O
была O
81.64 B-TERM
% I-TERM
) O
. O

# text =  Точность модели составила 58%, если учитывать все слот.
Точность B-TERM
модели O
составила O
58 B-TERM
% I-TERM
, O
если O
учитывать O
все O
слот O
. O

# text =  После тонкой настройки модель показала примерно 63%-ю точность как на учебном, так и на контрольном наборах данных.
После O
тонкой O
настройки O
модель O
показала O
примерно O
63%-ю B-TERM
точность B-TERM
как O
на O
учебном O
, O
так O
и O
на O
контрольном O
наборах O
данных O
. O

# text =  Самые интересные для нас сущности – судья и прокурор – быстро идентифицируются из более чем 200 миллионов документов с точностью выше 92 %».
Самые O
интересные O
для O
нас O
сущности O
– O
судья O
и O
прокурор O
– O
быстро O
идентифицируются O
из O
более O
чем O
200 O
миллионов O
документов O
с O
точностью B-TERM
выше O
92 B-TERM
%  I-TERM
» O
. O

# text =  В сравнении показываются FriendBERT и ChatBERT, которые по итогу исследования представили точность (MacroAVG F-меру), равную 73% для первой и 69,5% для второй модели соответственно.
В O
сравнении O
показываются O
FriendBERT B-TERM
и O
ChatBERT B-TERM
, O
которые O
по O
итогу O
исследования O
представили O
точность B-TERM
( O
MacroAVG B-TERM
F I-TERM
- I-TERM
меру I-TERM
) O
, O
равную O
73 B-TERM
% I-TERM
для O
первой O
и O
69,5 B-TERM
% I-TERM
для O
второй O
модели O
соответственно O
. O

# text =  Разработал систему компьютерной алгебры Mathematica и систему извлечения знаний WolframAlpha.
Разработал O
систему O
компьютерной O
алгебры O
Mathematica B-TERM
и O
систему O
извлечения O
знаний O
WolframAlpha B-TERM
. O

# text =  В чем проблема Пролога, да и любой системы / языка программирования, назначение которых анализировать факты и искать ответы на вопросы?
В O
чем O
проблема O
Пролога B-TERM
, O
да O
и O
любой O
системы O
/ O
языка O
программирования O
, O
назначение O
которых O
анализировать O
факты O
и O
искать O
ответы O
на O
вопросы O
? O

# text =  Интерес представляют и системы DeepMind, которые в дополнение к нейросети имеют внешнюю память фактов (или опыта), что позволяет им обучаться без учителя «правилам игры», просто проявляя активность в среде и записывая ее результат.
Интерес O
представляют O
и O
системы O
DeepMind B-TERM
, O
которые O
в O
дополнение O
к O
нейросети O
имеют O
внешнюю O
память O
фактов O
( O
или O
опыта O
) O
, O
что O
позволяет O
им O
обучаться O
без O
учителя O
« O
правилам O
игры O
» O
, O
просто O
проявляя O
активность O
в O
среде O
и O
записывая O
ее O
результат O
. O

# text =  В заглавии упомянута и на картинке представлена ELIZA — диалоговая система-психоаналитик (сейчас, ее назвали бы чат-бот), родом из 60-ых годов.
В O
заглавии O
упомянута O
и O
на O
картинке O
представлена O
ELIZA B-TERM
— O
диалоговая O
система O
- O
психоаналитик O
( O
сейчас O
, O
ее O
назвали O
бы O
чат O
- O
бот O
) O
, O
родом O
из O
60-ых O
годов O
. O

# text = GNMT есть система нейронного машинного перевода (NMT) компании Google, которая использует нейросеть (ANN) для повышения точности и скорости перевода, и в частности для создания лучших, более естественных вариантов перевода текста в Google Translate.
GNMT B-TERM
есть O
система O
нейронного O
машинного O
перевода O
( O
NMT B-TERM
) O
компании O
Google B-TERM
, O
которая O
использует O
нейросеть O
( O
ANN B-TERM
) O
для O
повышения O
точности O
и O
скорости O
перевода O
, O
и O
в O
частности O
для O
создания O
лучших O
, O
более O
естественных O
вариантов O
перевода O
текста O
в O
Google B-TERM
Translate I-TERM
. O

# text =  Его обучали с помощью большого количества текстов из Интернета и системы Reinforcement Learning from Human Feedback.
Его O
обучали O
с O
помощью O
большого O
количества O
текстов O
из O
Интернета O
и O
системы O
Reinforcement B-TERM
Learning I-TERM
from I-TERM
Human I-TERM
Feedback I-TERM
. O

# text =  Во-вторых, даже перевод между русским и эрзянским ещё расти и расти: у SOTA систем для высокоресурсных языков BLEU обычно где-то между 40 и 80.
Во O
- O
вторых O
, O
даже O
перевод O
между O
русским O
и O
эрзянским O
ещё O
расти O
и O
расти O
: O
у O
SOTA B-TERM
систем O
для O
высокоресурсных O
языков O
BLEU B-TERM
обычно O
где O
- O
то O
между O
40 O
и O
80 O
. O

# text =  Insertions – вставки слов, которых нет в исходной аудиозаписи Substitutions – замены слов на некорректные  Deletions – система слово не распознала и сделала пропуск 
Insertions O
– O
вставки O
слов O
, O
которых O
нет O
в O
исходной O
аудиозаписи O
Substitutions O
– O
замены O
слов O
на O
некорректные O
  O
Deletions B-TERM
– O
система O
слово O
не O
распознала O
и O
сделала O
пропуск O

# text = “Вам курицу или рыбу?” – Рекомендательная система на “Своем Родном” знает ответ / Habr
“ O
Вам O
курицу O
или O
рыбу O
? O
” O
– O
Рекомендательная O
система O
на O
“ O
Своем B-TERM
Родном I-TERM
” O
знает O
ответ O

# text =  Его обучали с помощью большого количества текстов из Интернета и системы Reinforcement Learning from Human Feedback.
Его O
обучали O
с O
помощью O
большого O
количества O
текстов O
из O
Интернета O
и O
системы O
Reinforcement B-TERM
Learning I-TERM
from I-TERM
Human I-TERM
Feedback I-TERM
. O

# text =  Система Kaldi, разработанная британским специалистом по нейросетям Даниэлем Повеем, предоставляет пользователю наиболее широкий выбор алгоритмов для разных задач и удобна в использовании.
Система O
Kaldi B-TERM
, O
разработанная O
британским O
специалистом O
по O
нейросетям O
Даниэлем B-TERM
Повеем I-TERM
, O
предоставляет O
пользователю O
наиболее O
широкий O
выбор O
алгоритмов O
для O
разных O
задач O
и O
удобна O
в O
использовании O
. O

# text =  Британский специалист по нейросетям Даниэль Пове создал систему Kaldi, которая предоставляет пользователю разнообразные алгоритмы для решения различных задач и отличается удобством в использовании.
Британский O
специалист O
по O
нейросетям O
Даниэль B-TERM
Пове I-TERM
создал O
систему O
Kaldi B-TERM
, O
которая O
предоставляет O
пользователю O
разнообразные O
алгоритмы O
для O
решения O
различных O
задач O
и O
отличается O
удобством O
в O
использовании O
. O

# text =  А AI позволяет извлекать из данных нужную информацию, что делает системы IoT намного более интеллектуальными.
А O
AI O
позволяет O
извлекать O
из O
данных O
нужную O
информацию O
, O
что O
делает O
системы O
IoT B-TERM
намного O
более O
интеллектуальными O
. O

# text =  После того, как данные загружены в систему, CLI позволяет осуществить её тонкую настройку, в том числе — воспользоваться специальной опцией для двоичной классификации.
После O
того O
, O
как O
данные O
загружены O
в O
систему O
, O
CLI B-TERM
позволяет O
осуществить O
её O
тонкую O
настройку O
, O
в O
том O
числе O
— O
воспользоваться O
специальной O
опцией O
для O
двоичной O
классификации O
. O

# text = Google представила систему искусственного интеллекта MusicLM, которая способна генерировать музыку в любом жанре по текстовому описанию.
Google B-TERM
представила O
систему O
искусственного O
интеллекта O
MusicLM B-TERM
, O
которая O
способна O
генерировать O
музыку O
в O
любом O
жанре O
по O
текстовому O
описанию O
. O

# text =  Ниже показаны примеры генерации кода с использованием сервиса CodeWhisperer на всех трёх перечисленных выше языках программирования.
Ниже O
показаны O
примеры O
генерации O
кода O
с O
использованием O
сервиса O
CodeWhisperer B-TERM
на O
всех O
трёх O
перечисленных O
выше O
языках O
программирования O
. O

# text = Аналогичный описанному выше сервис был запущен Microsoft в прошлом, 2021 году, и получил название Copilot.
Аналогичный O
описанному O
выше O
сервис O
был O
запущен O
Microsoft B-TERM
в O
прошлом O
, O
2021 B-TERM
году O
, O
и O
получил O
название O
Copilot B-TERM
. O

# text =  Система может быть подключена в виде расширения для сред разработки: Visual Studio Code, Visual Studio, Neovim, набора IDE от JetBrains.
Система O
может O
быть O
подключена O
в O
виде O
расширения O
для O
сред O
разработки O
: O
Visual B-TERM
Studio I-TERM
Code I-TERM
, O
Visual B-TERM
Studio I-TERM
, O
Neovim B-TERM
, O
набора O
IDE B-TERM
от O
JetBrains B-TERM
. O

# text =  Процент роботов среди трафика относительно низок, при этом самый высокий показатель приходится на трафик поисковых систем (Baidu) и составляет 1,0%.
Процент O
роботов O
среди O
трафика O
относительно O
низок O
, O
при O
этом O
самый O
высокий O
показатель O
приходится O
на O
трафик O
поисковых O
систем O
( O
Baidu B-TERM
) O
и O
составляет O
1,0%.

# text =  Поддерживается система рекомендаций для разработки приложений на языках Java, Javascript и Python.
Поддерживается O
система O
рекомендаций O
для O
разработки O
приложений O
на O
языках O
Java B-TERM
, O
Javascript B-TERM
и O
Python B-TERM
. O

# text =  Ниже показаны примеры генерации кода с использованием сервиса CodeWhisperer на всех трёх перечисленных выше языках программирования.
Ниже O
показаны O
примеры O
генерации O
кода O
с O
использованием O
сервиса O
CodeWhisperer B-TERM
на O
всех O
трёх O
перечисленных O
выше O
языках O
программирования O
. O

# text = Одним из самых известных датасетов по задаче модерации является датасет с соревнования на Kaggle Toxic Comment Classification Challenge.
Одним O
из O
самых O
известных O
датасетов O
по O
задаче O
модерации O
является O
датасет B-TERM
с O
соревнования O
на O
Kaggle B-TERM
Toxic I-TERM
Comment I-TERM
Classification I-TERM
Challenge I-TERM
. O

# text =  Для её обучения был использован гигантский датасет mC4, включающий в себя 6,6 млрд веб-страниц на 101 языке.
Для O
её O
обучения B-TERM
был O
использован O
гигантский O
датасет O
mC4 B-TERM
, O
включающий O
в O
себя O
6,6 O
млрд O
веб O
- O
страниц O
на O
101 O
языке O
. O

# text =  Есть множество датасетов, таких как:Paraphraser Plus; корпус парафраз, собранных Давидом Дале;корпус парафраз из новостных заголовков, собранный Екатериной Пронозой;корпус парафраз-заголовков, собранный командой Вадима Гудкова.
Есть O
множество O
датасетов O
, O
таких O
как O
: O
Paraphraser B-TERM
Plus I-TERM
; O
корпус B-TERM
парафраз I-TERM
, O
собранных O
Давидом B-TERM
Дале I-TERM
; O
корпус B-TERM
парафраз I-TERM
из I-TERM
новостных I-TERM
заголовков I-TERM
, O
собранный O
Екатериной B-TERM
Пронозой I-TERM
; O
корпус B-TERM
парафраз I-TERM
- I-TERM
заголовков I-TERM
, O
собранный O
командой O
Вадима B-TERM
Гудкова I-TERM
. O

# text =  На базе этих корпусов есть также обученные модели в том числе на основе ruT5 и ruGPT3 (например, несколько моделей находятся в библиотеке russian_paraphrases, или например мультитасковая модель).
На O
базе O
этих O
корпусов B-TERM
есть O
также O
обученные O
модели O
в O
том O
числе O
на O
основе O
ruT5 B-TERM
и O
ruGPT3 B-TERM
( O
например O
, O
несколько O
моделей O
находятся O
в O
библиотеке O
russian_paraphrases B-TERM
, O
или O
например O
мультитасковая O
модель O
) O
. O

# text =  Прежде всего, стоит выделить Dialog State Tracking Challenge, в этом году он, кстати, будет проводиться уже в шестой раз.
Прежде O
всего O
, O
стоит O
выделить O
Dialog B-TERM
State I-TERM
Tracking I-TERM
Challenge I-TERM
, O
в O
этом O
году O
он O
, O
кстати O
, O
будет O
проводиться O
уже O
в O
шестой O
раз O
. O

# text = Текстах аудиокниг, будем использовать датасет caito, в котором как раз есть тексты на всех языках, на которых обучалась модель (20,000 случайных предложений на каждый язык);
Текстах O
аудиокниг O
, O
будем O
использовать O
датасет O
caito B-TERM
, O
в O
котором O
как O
раз O
есть O
тексты O
на O
всех O
языках O
, O
на O
которых O
обучалась O
модель O
( O
20,000 O
случайных O
предложений O
на O
каждый O
язык O
) O
; O

# text =  Это не упоминалось ранее, однако исходные вопросы для сбора демонстраций и генерации пар на сравнение (в RL/RM частях) были выбраны из датасета ELI5.
Это O
не O
упоминалось O
ранее O
, O
однако O
исходные O
вопросы O
для O
сбора O
демонстраций O
и O
генерации O
пар O
на O
сравнение O
( O
в O
RL O
/ O
RM O
частях O
) O
были O
выбраны O
из O
датасета O
ELI5 B-TERM
. O

# text =  Пары предложений новых языков с русским я взял из датасета CCMatrix, и по ходу дообучения дополнял их эрзянскими переводами из своей русско-эрзянской модели, чтобы потом учить другую модель переводить эти эрзянские тексты на иностранный.
Пары O
предложений O
новых O
языков O
с O
русским O
я O
взял O
из O
датасета O
CCMatrix B-TERM
, O
и O
по O
ходу O
дообучения O
дополнял O
их O
эрзянскими O
переводами O
из O
своей O
русско O
- O
эрзянской O
модели O
, O
чтобы O
потом O
учить O
другую O
модель O
переводить O
эти O
эрзянские O
тексты O
на O
иностранный O
. O

# text =  Вот несколько примеров из тренировочного датасета:ContextQuestionAnswer
Вот O
несколько O
примеров O
из O
тренировочного O
датасета O
: O
ContextQuestionAnswer B-TERM

# text =  Например, в наборе данных CRD3 использовались стенограммы шоу Critical Role.
Например O
, O
в O
наборе O
данных O
CRD3 B-TERM
использовались O
стенограммы O
шоу O
Critical O
Role O
. O

# text =  Проверим этот метод на практике, обучив модель на табличном датасете California Housing, в котором нужно предсказывать цену недвижимости в разных районах Калифорнии, имея 8 исходных признаков.
Проверим O
этот O
метод O
на O
практике O
, O
обучив B-TERM
модель B-TERM
на O
табличном O
датасете O
California B-TERM
Housing I-TERM
, O
в O
котором O
нужно O
предсказывать O
цену O
недвижимости O
в O
разных O
районах O
Калифорнии O
, O
имея O
8 O
исходных O
признаков O
. O

# text =  Давайте протестируем данную методику, обучив модель на датасете California Housing.
Давайте O
протестируем O
данную O
методику O
, O
обучив B-TERM
модель B-TERM
на O
датасете O
California B-TERM
Housing I-TERM
. O

# text = Чтобы оценить точность регрессора, мы будем использовать наборы данных Medical Cost Personal Datasets | Kaggle.
Чтобы O
оценить B-TERM
точность I-TERM
регрессора I-TERM
, O
мы O
будем O
использовать O
наборы O
данных O
Medical B-TERM
Cost I-TERM
Personal I-TERM
Datasets I-TERM
Kaggle I-Dataset
. O

# text = В задаче классификации на 10%, 20%, 30%, 40%, 50% от общего датасета тестовой выборки DatRet показал лучшие результаты.
В O
задаче O
классификации O
на O
10 O
% O
, O
20 O
% O
, O
30 O
% O
, O
40 O
% O
, O
50 O
% O
от O
общего O
датасета O
тестовой O
выборки O
DatRet B-TERM
показал O
лучшие O
результаты O
. O

# text =  И речь здесь даже не о том, что метрики качества оцениваются в первую очередь на общих датасетах вроде COCO, а в том, что сами метрики заточены под исследовательские цели.
И O
речь O
здесь O
даже O
не O
о O
том O
, O
что O
метрики O
качества O
оцениваются O
в O
первую O
очередь O
на O
общих O
датасетах O
вроде O
COCO B-TERM
, O
а O
в O
том O
, O
что O
сами O
метрики O
заточены O
под O
исследовательские O
цели O
. O

# text =  TAPE является логичным развитием проекта Russian SuperGLUE, где на вопросно-ответных датасетах RuCoS, MuSeRC и DaNetQA решения участников уже достигли уровня человека.
TAPE B-TERM
является O
логичным O
развитием O
проекта O
Russian B-TERM
SuperGLUE I-TERM
, O
где O
на O
вопросно O
- O
ответных O
датасетах O
RuCoS B-TERM
, O
MuSeRC B-TERM
и O
DaNetQA B-TERM
решения O
участников O
уже O
достигли O
уровня O
человека O
. O

# text =  Датасет Ethics состоит из двух частей.
Датасет O
Ethics B-TERM
состоит O
из O
двух O
частей O
. O

# text =  Для тестирования были отобраны два класса по две тысячи примеров из датасета интентов нашего чат-бота Смарти, которые показывали высокую и среднюю оценку разнообразия по self-bleu по 3-граммам.
Для O
тестирования O
были O
отобраны O
два O
класса O
по O
две O
тысячи O
примеров O
из O
датасета B-TERM
интентов I-TERM
нашего O
чат O
- O
бота O
Смарти B-TERM
, O
которые O
показывали O
высокую O
и O
среднюю O
оценку O
разнообразия O
по O
self O
- O
bleu O
по O
3-граммам O
. O

# text =  В рамках этой статьи давайте предположим, что у нас есть датасет для обучения обычного многоклассового классификатора интентов.
В O
рамках O
этой O
статьи O
давайте O
предположим O
, O
что O
у O
нас O
есть O
датасет B-TERM
для I-TERM
обучения I-TERM
обычного I-TERM
многоклассового I-TERM
классификатора I-TERM
интентов I-TERM
. O

# text =  Обучались на датасете открытого кода с Гитхаба, больше всего в выборке было питона.
Обучались O
на O
датасете B-TERM
открытого I-TERM
кода I-TERM
с I-TERM
Гитхаба I-TERM
, O
больше O
всего O
в O
выборке O
было O
питона O
. O

# text =  Данные для обучения включали в себя отфильтрованный датасет CommonCrawl (составляет большую пропорцию всех текстов, которые присутствовали в обучении), а также корпус книжных текстов и текстов Википедии.
Данные O
для O
обучения O
включали O
в O
себя O
отфильтрованный O
датасет O
CommonCrawl B-TERM
( O
составляет O
большую O
пропорцию O
всех O
текстов O
, O
которые O
присутствовали O
в O
обучении O
) O
, O
а O
также O
корпус O
книжных O
текстов O
и O
текстов O
Википедии B-TERM
. O

# text =  Dusha: самый большой открытый датасет для распознавания эмоций в устной речи на русском языке.
Dusha B-TERM
: O
самый O
большой O
открытый O
датасет B-TERM
для O
распознавания B-TERM
эмоций I-TERM
в O
устной O
речи O
на O
русском B-TERM
языке I-TERM
. O

# text =  Dusha - это крупнейший открытый набор данных на русском языке для распознавания эмоций в речи.
Dusha B-TERM
- O
это O
крупнейший O
открытый O
набор B-TERM
данных I-TERM
на O
русском B-TERM
языке I-TERM
для O
распознавания B-TERM
эмоций I-TERM
в I-TERM
речи I-TERM
. O

# text =  Де-факто, в подавляющем большинстве случаев, бенчмарком для новых моделей распознавания эмоций является англоязычный датасет IEMOCAP с игрой профессиональных актёров. 
Де O
- O
факто O
, O
в O
подавляющем O
большинстве O
случаев O
, O
бенчмарком O
для O
новых O
моделей O
распознавания B-TERM
эмоций I-Task
является O
англоязычный B-TERM
датасет O
IEMOCAP B-TERM
с O
игрой O
профессиональных O
актёров O
. O

# text =  IEMOCAP, англоязычный набор данных, служит стандартом для распознавания эмоций.
IEMOCAP B-TERM
, O
англоязычный B-TERM
набор O
данных O
, O
служит O
стандартом O
для O
распознавания B-TERM
эмоций I-Task
. O

# text =  Если датасеты с большим количеством семплов и находятся (к примеру, CMU-MOSEI, MURCO), то у них очень ярко проявляется проблема из п1.
Если O
датасеты O
с O
большим O
количеством O
семплов O
и O
находятся O
( O
к O
примеру O
, O
CMU B-TERM
- I-TERM
MOSEI I-TERM
, O
MURCO B-TERM
) O
, O
то O
у O
них O
очень O
ярко O
проявляется O
проблема O
из O
п1.

# text =  Столкнувшись с описанными выше проблемами, мы решили собрать свой датасет для распознавания эмоций и назвали его Dusha, по аналогии с датасетом для распознавания речи — Golos.
Столкнувшись O
с O
описанными O
выше O
проблемами O
, O
мы O
решили O
собрать O
свой O
датасет O
для O
распознавания B-TERM
эмоций I-TERM
и O
назвали O
его O
Dusha B-TERM
, O
по O
аналогии O
с O
датасетом O
для O
распознавания B-TERM
речи I-TERM
— O
Golos B-TERM
. O

# text =  Эту часть датасета мы назвали Crowd.
Эту O
часть O
датасета O
мы O
назвали O
Crowd B-TERM
. O

# text =  Мы обучали движок Amazon Translate, используя «Active Custom Translation», который позволяет выполнять перевод на лету с использованием двуязычного корпуса.
Мы O
обучали O
движок O
Amazon B-TERM
Translate I-TERM
, O
используя O
« O
Active B-TERM
Custom I-TERM
Translation I-TERM
» O
, O
который O
позволяет O
выполнять O
перевод B-TERM
на O
лету O
с O
использованием O
двуязычного B-TERM
корпуса I-TERM
. O

# text = Та статья была написана совместно с Ильей Гусевым, у которого есть библиотека для анализа и генерации стихов на русском языке и поэтический корпус русского языка.
Та O
статья O
была O
написана O
совместно O
с O
Ильей B-TERM
Гусевым I-TERM
, O
у O
которого O
есть O
библиотека B-TERM
для I-TERM
анализа I-TERM
и I-TERM
генерации I-TERM
стихов I-TERM
на O
русском B-TERM
языке I-TERM
и O
поэтический B-TERM
корпус I-TERM
русского B-TERM
языка I-TERM
. O

# text = Этот материал был создан совместно с Ильей Гусевым, у которого имеется библиотека для анализа и генерации стихов на русском языке, а также поэтический корпус русского языка.
Этот O
материал O
был O
создан O
совместно O
с O
Ильей B-TERM
Гусевым I-TERM
, O
у O
которого O
имеется O
библиотека B-TERM
для I-TERM
анализа I-TERM
и I-TERM
генерации I-TERM
стихов I-TERM
на O
русском B-TERM 
языке I-TERM 
, O
а O
также O
поэтический B-TERM
корпус I-TERM
русского B-TERM
языка I-TERM
. O

# text =  Обученный на большом корпусе русской литературы «Порфирьевич» порадовал публику множеством забавных творений.
Обученный O
на O
большом O
корпусе B-TERM
русской I-TERM
литературы I-TERM
« O
Порфирьевич B-TERM
» O
порадовал O
публику O
множеством O
забавных O
творений O
. O

# text =  Для обучения использовался корпус объёмом около 750 Гб, получивший название C4 (Colossal Clean Crawled Corpus, Колоссальный очищенный собранный в интернете корпус), являющийся отфильтрованной версией корпуса Common Crawl.
Для O
обучения O
использовался O
корпус O
объёмом O
около O
750 O
Гб O
, O
получивший O
название O
C4 B-TERM
( O
Colossal B-TERM
Clean I-TERM
Crawled I-TERM
Corpus I-TERM
, O
Колоссальный B-TERM
очищенный I-TERM
собранный I-TERM
в I-TERM
интернете I-TERM
корпус I-TERM
) O
, O
являющийся O
отфильтрованной O
версией O
корпуса O
Common B-TERM
Crawl I-TERM
. O

# text =  Для обучения использовался корпус C4 (Colossal Clean Crawled Corpus, Колоссальный очищенный собранный в интернете корпус). 
Для O
обучения O
использовался O
корпус O
C4 B-TERM
( O
Colossal B-TERM
Clean I-TERM
Crawled I-TERM
Corpus I-TERM
, O
Колоссальный B-TERM
очищенный I-TERM
собранный I-TERM
в I-TERM
интернете I-TERM
корпус I-TERM
) O
. O

# text =  Для ещё двух языков, английского и финского, несколько сотен параллельных предложений нашлось в эрзянском корпусе Universal Dependencies, собранном всё тем же Jack'ом Rueter'ом.
Для O
ещё O
двух O
языков O
, O
английского B-TERM
и O
финского B-TERM
, O
несколько O
сотен O
параллельных O
предложений O
нашлось O
в O
эрзянском B-TERM
корпусе O
Universal B-TERM
Dependencies I-TERM
, O
собранном O
всё O
тем O
же O
Jack'ом B-TERM
Rueter'ом I-TERM
. O

# text =  В качестве данных используется корпус журналов из области астрономии.
В O
качестве O
данных O
используется O
корпус B-TERM
журналов I-TERM
из O
области O
астрономии B-TERM
. O

# text =  Они обнаружили, что логистическая регрессия с содержательными признаками (content-based features), полученными на основе близости тем и слов в корпусе ACL (корпусе научных публикаций на английском языке), даёт наилучший результат.
Они O
обнаружили O
, O
что O
логистическая B-TERM
регрессия I-TERM
с O
содержательными O
признаками O
( O
content O
- O
based O
features O
) O
, O
полученными O
на O
основе O
близости O
тем O
и O
слов O
в O
корпусе O
ACL B-TERM
( O
корпусе B-TERM
научных I-TERM
публикаций I-TERM
на O
английском B-TERM
языке I-TERM
) O
, O
даёт O
наилучший O
результат O
. O

# text =  Использование логистической регрессии на корпусе ACL (корпусе научных публикаций на английском языке) даёт наилучший результат. 
Использование O
логистической B-TERM
регрессии I-TERM
на O
корпусе O
ACL B-TERM
( O
корпусе B-TERM
научных I-TERM
публикаций I-TERM
на O
английском B-TERM
языке I-TERM
) O
даёт O
наилучший O
результат O
. O

# text =  Например, при изучении развёрнутой модели на 16 итераций на примере корпуса книг о Шерлоке Холмсе после 60 000 итераций (обучение на примерно 1 Мб текста) она выдаёт довольно бессмысленный текст.
Например O
, O
при O
изучении O
развёрнутой O
модели O
на O
16 O
итераций O
на O
примере O
корпуса B-TERM
книг I-TERM
о I-TERM
Шерлоке I-TERM
Холмсе I-TERM
после O
60 O
000 O
итераций O
( O
обучение O
на O
примерно O
1 O
Мб O
текста O
) O
она O
выдаёт O
довольно O
бессмысленный O
текст O
. O

# text =  В нашем случае это был корпус текстов из социальных сетей с 48 млн.
В O
нашем O
случае O
это O
был O
корпус B-TERM
текстов I-TERM
из I-TERM
социальных I-TERM
сетей I-TERM
с O
48 O
млн O
. O

# text =  Первое подобное исследование решает задачу автоматической оценки приемлемости в русском языке на основе корпуса предложений из лингвистических статей, что вызывает две проблемы.
Первое O
подобное O
исследование O
решает O
задачу O
автоматической B-TERM
оценки I-TERM
приемлемости I-TERM
в O
русском B-TERM
языке I-TERM
на O
основе O
корпуса B-TERM
предложений I-TERM
из I-TERM
лингвистических I-TERM
статей I-TERM
, O
что O
вызывает O
две O
проблемы O
. O

# text =  В качестве данных был выбран открытый корпус русскоязычных твитов.
В O
качестве O
данных O
был O
выбран O
открытый B-TERM
корпус I-TERM
русскоязычных I-TERM
твитов I-TERM
. O

# text =  Отличием self-bleu от оригинала заключается в том, что вместо референтного корпуса используются все сгенерированные тексты, кроме тестируемого.
Отличием O
self O
- O
bleu O
от O
оригинала O
заключается O
в O
том O
, O
что O
вместо O
референтного B-TERM
корпуса I-TERM
используются O
все O
сгенерированные O
тексты O
, O
кроме O
тестируемого O
. O

# text =  Таким образом, чем больше совпадений н-грамм между референтным корпусом и проверяемым текстом, тем выше значение этой метрики.
Таким O
образом O
, O
чем O
больше O
совпадений O
н B-TERM
- I-TERM
грамм I-TERM
между O
референтным B-TERM
корпусом I-TERM
и O
проверяемым O
текстом O
, O
тем O
выше O
значение O
этой O
метрики O
. O

# text = Хочу показать, как создать мультиязычный параллельный корпус и книги при помощи пет-проекта, которым я занимаюсь несколько лет.
Хочу O
показать O
, O
как O
создать O
мультиязычный B-TERM
параллельный I-TERM
корпус I-TERM
и O
книги O
при O
помощи O
пет O
- O
проекта O
, O
которым O
я O
занимаюсь O
несколько O
лет O
. O

# text =  Получим параллельный корпус на 10 языках и много красивых книг.
Получим O
параллельный B-TERM
корпус I-TERM
на O
10 O
языках O
и O
много O
красивых O
книг O
. O

# text =  Я использовал модель, обученную на Национальном Корпусе Русского Языка (НКРЯ), ее название — ruscorpora_upos_cbow_300_20_2019.
Я O
использовал O
модель B-TERM
, O
обученную B-TERM
на O
Национальном B-TERM
Корпусе I-TERM
Русского I-TERM
Языка I-TERM
( O
НКРЯ B-TERM
) O
, O
ее O
название O
— O
ruscorpora_upos_cbow_300_20_2019 B-TERM
. O

# text =  Для наших целей прекрасно подошла модель ruscorpora_upos_cbow_300_20_2019, обученная на Национальном Корпусе Русского Языка (НКРЯ). 
Для O
наших O
целей O
прекрасно O
подошла O
модель O
ruscorpora_upos_cbow_300_20_2019 B-TERM
, O
обученная B-TERM
на O
Национальном B-TERM
Корпусе I-TERM
Русского I-TERM
Языка I-TERM
( O
НКРЯ B-TERM
) O
. O

# text =  Для своих целей я применил модель, обученную на Национальном Корпусе Русского Языка (НКРЯ) под названием ruscorpora_upos_cbow_300_20_2019.
Для O
своих O
целей O
я O
применил O
модель O
, O
обученную O
на O
Национальном B-TERM
Корпусе I-TERM
Русского I-TERM
Языка I-TERM
( O
НКРЯ B-TERM
) O
под O
названием O
ruscorpora_upos_cbow_300_20_2019 B-TERM
. O

# text =  Основной тип данных для обучения переводчика — это bitext-корпусы, состоящие из пар текстов «оригинал — перевод».
Основной O
тип O
данных O
для O
обучения O
переводчика O
— O
это O
bitext B-TERM
- I-TERM
корпусы I-TERM
, O
состоящие O
из O
пар O
текстов O
« O
оригинал O
— O
перевод O
» O
. O

# text =  Например, один из крупнейших корпусов UNPC состоит из официальных и юридических текстов.
Например O
, O
один O
из O
крупнейших O
корпусов O
UNPC B-TERM
состоит O
из O
официальных O
и O
юридических O
текстов O
. O

# text =  Другой полезный источник — это mono-корпусы, состоящие из большого объёма обычных текстов.
Другой O
полезный O
источник O
— O
это O
mono B-TERM
- I-TERM
корпусы I-TERM
, O
состоящие O
из O
большого O
объёма O
обычных O
текстов O
. O

# text =  На основе mono-корпусов мы предобучаем разные вспомогательные модели, начиная с токенизаторов и заканчивая большими языковыми претрейнами типа BART и T5.
На O
основе O
mono B-TERM
- I-TERM
корпусов I-TERM
мы O
предобучаем B-TERM
разные O
вспомогательные O
модели O
, O
начиная O
с O
токенизаторов O
и O
заканчивая O
большими O
языковыми O
претрейнами O
типа O
BART B-TERM
и O
T5 B-TERM
. O

# text =  Исходя из моно-корпусов, мы проводим предварительное обучение различных вспомогательных моделей, начиная с токенизаторов и заканчивая обширными языковыми предварительными тренировками, такими как BART и T5.
Исходя O
из O
моно B-TERM
- I-TERM
корпусов I-TERM
, O
мы O
проводим O
предварительное O
обучение B-TERM
различных O
вспомогательных O
моделей O
, O
начиная O
с O
токенизаторов O
и O
заканчивая O
обширными O
языковыми O
предварительными O
тренировками O
, O
такими O
как O
BART B-TERM
и O
T5 B-TERM
. O

# text =  Авторы, предложившие такой подход, делали свой поиск внутри огромного корпуса Common Crawl и снапшотов Википедии, и поделились с нами новыми крупными датасетами CCMatrix и WikiMatrix.
Авторы O
, O
предложившие O
такой O
подход O
, O
делали O
свой O
поиск O
внутри O
огромного O
корпуса O
Common B-TERM
Crawl I-TERM
и O
снапшотов O
Википедии B-TERM
, O
и O
поделились O
с O
нами O
новыми O
крупными O
датасетами O
CCMatrix B-TERM
и O
WikiMatrix B-TERM
. O

# text =  Для сравнения, размер хорошего английского корпуса The Pile составляет более 800Гб.
Для O
сравнения O
, O
размер O
хорошего O
английского B-TERM
корпуса O
The B-TERM
Pile I-TERM
составляет O
более O
800 O
Гб O
. O

# text =  В конце концов, основным в обучении GPT был корпус англоязычной литературы.
В O
конце O
концов O
, O
основным O
в O
обучении O
GPT B-TERM
был O
корпус B-TERM
англоязычной I-TERM
литературы I-TERM
. O

# text =  Я решил велосипед не изобретать и собрать свой датасет из готовых дампов Википедии, новостей, выгрузки постов с Habr и корпуса русских книг, отсекая тексты короче 10000 токенов.
Я O
решил O
велосипед O
не O
изобретать O
и O
собрать O
свой O
датасет O
из O
готовых O
дампов O
Википедии B-TERM
, O
новостей O
, O
выгрузки O
постов O
с O
Habr B-TERM
и O
корпуса B-TERM
русских I-TERM
книг I-TERM
, O
отсекая O
тексты O
короче O
10000 O
токенов O
. O

# text = После преобразования сообщений в векторы можно использовать любой классический метод для классификации: логистическую регрессию, SVM, случайный лес, бустинг.
После O
преобразования O
сообщений O
в O
векторы O
можно O
использовать O
любой O
классический O
метод O
для O
классификации B-TERM
: O
логистическую B-TERM
регрессию B-TERM
, O
SVM B-TERM
, O
случайный B-TERM
лес I-TERM
, O
бустинг B-TERM
. O

# text =  Для сверточных нейросетей хорошо настроенный метод стохастического градиента (SGD) почти всегда немного превосходит Adam, но область оптимальной скорости обучения гораздо более узкая и зависит от задачи.
Для O
сверточных O
нейросетей O
хорошо O
настроенный O
метод B-TERM
стохастического I-TERM
градиента I-TERM
( O
SGD B-TERM
) O
почти O
всегда O
немного O
превосходит O
Adam B-TERM
, O
но O
область O
оптимальной O
скорости O
обучения O
гораздо O
более O
узкая O
и O
зависит O
от O
задачи O
. O

# text =  После работы Эдсгера Дейкстры «О вреде оператора goto», давшей отправную точку парадигмы "структурного программирования", было немало работ, посвященных "структуре алгоритма": методология ООП, функциональное программирование, паттерны проектирования, принципы SOLID…
После O
работы O
Эдсгера B-TERM
Дейкстры I-TERM
« O
О O
вреде O
оператора O
goto O
» O
, O
давшей O
отправную O
точку O
парадигмы O
" O
структурного O
программирования O
" O
, O
было O
немало O
работ O
, O
посвященных O
" O
структуре O
алгоритма O
" O
: O
методология O
ООП B-TERM
, O
функциональное B-TERM
программирование I-TERM
, O
паттерны B-TERM
проектирования I-TERM
, O
принципы B-TERM
SOLID I-TERM
… O

# text =  В случае GNMT речь идет о так называемом методе перевода на основе примеров (EBMT), т.е.
В O
случае O
GNMT B-TERM
речь O
идет O
о O
так O
называемом O
методе B-TERM
перевода I-TERM
на I-TERM
основе I-TERM
примеров I-TERM
( O
EBMT B-TERM
) O
, O
т.е. O

# text =  Мы обучаем методологии CRISP-DM, учим постановке гипотез, выбору и аргументации методов исследования, интерпретации и представлению результатов.
Мы O
обучаем O
методологии O
CRISP B-TERM
- I-TERM
DM I-TERM
, O
учим O
постановке O
гипотез O
, O
выбору O
и O
аргументации O
методов O
исследования O
, O
интерпретации O
и O
представлению O
результатов O
. O

# text =  На момент создания в 2020 году такая модель была наикрупнейшей.
На O
момент O
создания O
в O
2020 B-TERM
году O
такая O
модель B-TERM
была O
наикрупнейшей O
. O

# text =  В 2020 году появился GPT-3.
В O
2020 B-TERM
году O
появился O
GPT-3 B-TERM
. O

# text =  О своём партнёрстве с Microsoft OpenAI объявила в конце 2019 года.
О O
своём O
партнёрстве O
с O
Microsoft B-TERM
OpenAI B-TERM
объявила O
в O
конце O
2019 B-TERM
года O
. O

# text = С появлением в 2020 году нейронной сети GPT3 и других архитектур – трансформеров, генерируемые тексты стали невероятно правдоподобными.
С O
появлением O
в O
2020 B-TERM
году O
нейронной O
сети O
GPT3 B-TERM
и O
других O
архитектур B-TERM
– I-TERM
трансформеров I-TERM
, O
генерируемые O
тексты O
стали O
невероятно O
правдоподобными O
. O

# text =  В 2020 компания приобрела эксклюзивную лицензию на базовую технологию, лежащую в основе GPT-3.
В O
2020 B-TERM
компания O
приобрела O
эксклюзивную O
лицензию O
на O
базовую O
технологию O
, O
лежащую O
в O
основе O
GPT-3 B-TERM
. O

# text =  6 февраля 2023 года Google представил свой аналог ChatGPT — экспериментальный диалоговый ИИ-сервис под названием Bard.
6 O
февраля O
2023 B-TERM
года O
Google B-TERM
представил O
свой O
аналог O
ChatGPT B-TERM
— O
экспериментальный O
диалоговый O
ИИ O
- O
сервис O
под O
названием O
Bard B-TERM
. O

# text =  Однако, по мере углубления в тему, автор связался с разработчиками и в январе 2023 года получил первую версию ScoreCloud Songwriter на тестирование под Windows 10.
Однако O
, O
по O
мере O
углубления O
в O
тему O
, O
автор O
связался O
с O
разработчиками O
и O
в O
январе O
2023 B-TERM
года O
получил O
первую O
версию O
ScoreCloud B-TERM
Songwriter I-TERM
на O
тестирование O
под O
Windows B-TERM
10 I-TERM
. O

# text =  В 2020 нашими коллегами из команды AGI NLP Сбербанка, лаборатории Noah’s Ark Huawei и факультета компьютерных наук ВШЭ был представлен Russian SuperGLUE — набор задач на понимание текста по аналогии с его английской версией SuperGLUE.
В O
2020 B-TERM
нашими O
коллегами O
из O
команды O
AGI B-TERM
NLP I-TERM
Сбербанка B-TERM
, O
лаборатории O
Noah B-TERM
’s I-TERM
Ark I-TERM
Huawei I-TERM
и O
факультета O
компьютерных O
наук O
ВШЭ B-TERM
был O
представлен O
Russian B-TERM
SuperGLUE I-TERM
— O
набор O
задач O
на O
понимание O
текста O
по O
аналогии O
с O
его O
английской O
версией O
SuperGLUE B-TERM
. O

# text =  Описание технологии появилось в общем доступе в 2020 году.
Описание O
технологии O
появилось O
в O
общем O
доступе O
  O
в O
2020 B-TERM
году O
. O

# text =  Обучение языковой модели происходило в 2021 году.
Обучение O
языковой O
модели O
происходило O
в O
2021 B-TERM
году O
. O

# text =  Первые нейронные сети были внедрены в кредитный скоринг в 2020-м году.
Первые O
нейронные O
сети O
были O
внедрены O
в O
кредитный O
скоринг O
в O
2020-м B-TERM
году O
. O

# text =  В 2021 году Amazon запустила SageMaker Studio — первый IDE для машинного обучения.
В O
2021 B-TERM
году O
Amazon B-TERM
запустила O
SageMaker B-TERM
Studio I-TERM
— O
первый O
IDE B-TERM
для O
машинного O
обучения O
. O

# text =  Создана в 2021 году.
Создана O
в O
2021 B-TERM
году O
. O

# text =  ChatGPT был запущен 30 ноября 2022 года и привлек внимание своими широкими возможностями: написание кода, создание текстов, возможности перевода, получения точных ответов и использование контекста диалога для ответов, хотя его фактическая точность подверглась критике (источник — Википедия).
ChatGPT B-TERM
был O
запущен O
30 B-TERM
ноября I-TERM
2022 I-TERM
года O
и O
привлек O
внимание O
своими O
широкими O
возможностями O
: O
написание B-TERM
кода I-TERM
, O
создание B-TERM
текстов I-TERM
, O
возможности O
перевода B-TERM
, O
получения B-TERM
точных I-TERM
ответов I-TERM
и O
использование O
контекста O
диалога O
для O
ответов O
, O
хотя O
его O
фактическая O
точность O
подверглась O
критике O
( O
источник O
— O
Википедия B-TERM
) O
. O

# text =  Компания уже давно работает над сложным поисковым ИИ под названием LaMDA: о нем впервые объявили еще в мае 2021 года.
Компания O
уже O
давно O
работает O
над O
сложным O
поисковым O
ИИ O
под O
названием O
LaMDA B-TERM
: O
о O
нем O
впервые O
объявили O
еще O
в O
мае O
2021 B-TERM
года O
. O

# text =  И уже в 2020-2021 годах компании начали запускать альтернативные платформы, а стартапы — внедрять Feature Store в свои проекты.
И O
уже O
в O
2020 B-TERM
- O
2021 B-TERM
годах O
компании O
начали O
запускать O
альтернативные O
платформы O
, O
а O
стартапы O
— O
внедрять O
Feature B-TERM
Store I-TERM
в O
свои O
проекты O
. O

# text =  На этапе обучения text-davinci-003 используются датасеты текстов и программного кода, собранные OpenAI на момент конца 2021 года.
На O
этапе O
обучения O
text B-TERM
- I-TERM
davinci-003 I-TERM
используются O
датасеты B-TERM
текстов I-TERM
и O
программного O
кода O
, O
собранные O
OpenAI B-TERM
на O
момент O
конца O
2021 B-TERM
года O
. O

# text =  На этапе обучения ChatGPT используются дополнительные текстовые данные и программный код, собранные на момент конца 2021 года.Reinforcement Learning with Human Feedback (RLHF)В основе лежит сильная предобученная языковая модель (в случае ChatGPT это InstructGPT, но могут быть и другие, например, Gopher от DeepMind).
На O
этапе O
обучения O
ChatGPT B-TERM
используются O
дополнительные O
текстовые O
данные O
и O
программный O
код O
, O
собранные O
на O
момент O
конца O
2021 B-TERM
года O
. O

# text =  Так и появился AIDungeon — уникальная для своего времени (2019 год) вещь, которая не сильно потеряла в популярности и по сей день.
Так O
и O
появился O
AIDungeon B-TERM
— O
уникальная O
для O
своего O
времени O
( O
2019 B-TERM
год O
) O
вещь O
, O
которая O
не O
сильно O
потеряла O
в O
популярности O
и O
по O
сей O
день O
. O

# text =  Окончательная версия TACL c достаточно хорошим внешним видом содержит около 150 цитат, связанных с BERT, и нет никаких иллюзий завершённости: в августе 2020 года у нас закончились отведённые для журнала страницы.
Окончательная O
версия O
TACL B-TERM
c O
достаточно O
хорошим O
внешним O
видом O
содержит O
около O
150 O
цитат O
, O
связанных O
с O
BERT B-TERM
, O
и O
нет O
никаких O
иллюзий O
завершённости O
: O
в O
августе O
2020 B-TERM
года O
у O
нас O
закончились O
отведённые O
для O
журнала O
страницы O
. O

# text =  Чтобы определить, какие проходы и MLP нужно обрезать, мы используем аппроксимацию, основанную на потерях: оценки важности, предложенные Michel, Levy and Neubig (2019) для проходов самонаблюдения, которые мы распространяем на MLP.
Чтобы O
определить O
, O
какие O
проходы O
и O
MLP B-TERM
нужно O
обрезать O
, O
мы O
используем O
аппроксимацию O
, O
основанную O
на O
потерях O
: O
оценки O
важности O
, O
предложенные O
Michel B-TERM
, O
Levy B-TERM
and O
Neubig B-TERM
( O
2019 B-TERM
) O
для O
проходов O
самонаблюдения O
, O
которые O
мы O
распространяем O
на O
MLP B-TERM
. O

# text =  Например, в 2022 году, помимо ABBYY, это будут МТС, SberDevices, Яндекс и другие.
Например O
, O
в O
2022 B-TERM
году O
, O
помимо O
ABBYY B-TERM
, O
это O
будут O
МТС B-TERM
, O
SberDevices B-TERM
, O
Яндекс B-TERM
и O
другие O
. O

# text =  Первая была в 2019 году, называлась AGRR: Automatic Gapping Resolution for Russian.
Первая O
была O
в O
2019 B-TERM
году O
, O
называлась O
AGRR B-TERM
: O
Automatic B-TERM
Gapping I-TERM
Resolution I-TERM
for I-TERM
Russian I-TERM
. O