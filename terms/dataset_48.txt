# text =  Наше выработанное решение – обучить нейронную сеть, которая способна по тексту обращения автоматически распознавать заранее ранжированные по классам проблемы, извлекать сущность (номер заказа и телефон клиента) и по определённым классам сделать автоматизацию решения.
Наше O
выработанное O
решение O
– O
обучить O
нейронную B-TERM
сеть I-TERM
, O
которая O
способна B-TERM
по O
тексту O
обращения O
автоматически B-TERM
распознавать I-TERM
заранее I-TERM
ранжированные I-TERM
по I-TERM
классам I-TERM
проблемы I-TERM
, O
извлекать B-TERM
сущность I-TERM
( O
номер O
заказа O
и O
телефон O
клиента O
) O
и O
по O
определённым O
классам O
сделать O
автоматизацию O
решения O
. O

# text =  На самом деле уже существуют продвинутые и проверенные методы ее обработки, использующие нейронные сети, с распознаванием смысла и контекста – BERT (Bidirectional Encoder Representations from Transformers).
На O
самом O
деле O
уже O
существуют O
продвинутые O
и O
проверенные O
методы O
ее O
обработки O
, O
использующие O
нейронные B-TERM
сети I-TERM
, O
с O
распознаванием B-TERM
смысла I-TERM
и I-TERM
контекста I-TERM
– O
BERT B-TERM
( O
Bidirectional B-TERM
Encoder I-TERM
Representations I-TERM
from I-TERM
Transformers I-TERM
) O
. O

# text =  Перед тем как выбрать нейронные сети, мы протестировали несколько более стандартных архитектур, случайные леса и бустинг.
Перед O
тем O
как O
выбрать O
нейронные B-TERM
сети I-TERM
, O
мы O
протестировали O
несколько O
более O
стандартных O
архитектур O
, O
случайные B-TERM
леса I-TERM
и O
бустинг B-TERM
. O

# text =  Эта модель была обучена на огромном корпусе русскоязычного текста с двумя задачами – предсказать замаскированное слово в предложениях и предсказать, если одно из предложений следует по смыслу за вторым.
Эта O
модель B-TERM
была O
обучена B-TERM
на I-TERM
огромном O
корпусе B-TERM
русскоязычного I-TERM
текста I-TERM
с O
двумя O
задачами O
– O
предсказать B-TERM
замаскированное I-TERM
слово I-TERM
в I-TERM
предложениях I-TERM
и O
предсказать O
, O
если O
одно O
из O
предложений O
следует O
по O
смыслу O
за O
вторым O
. O

# text =   На обширном корпусе русскоязычного текста данная модель была обучена выполнять две задачи: предсказывать замаскированные слова в предложениях и определять, следует ли одно предложение за другим по смыслу.
На O
обширном O
корпусе B-TERM
русскоязычного I-TERM
текста I-TERM
данная O
модель B-TERM
была O
обучена B-TERM
выполнять O
две O
задачи O
: O
предсказывать B-TERM
замаскированные I-TERM
слова I-TERM
в I-TERM
предложениях I-TERM
и O
определять O
, O
следует O
ли O
одно O
предложение O
за O
другим O
по O
смыслу O
. O

# text =  Наша задача – дообучить эту языковую модель для нашего приложения (одна модель для классификации и одна – для извлечения сущности).
Наша O
задача O
– O
дообучить O
эту O
языковую O
модель O
для O
нашего O
приложения O
( O
одна O
модель B-TERM
для B-TERM
классификации B-TERM
и O
одна O
– O
для B-TERM
извлечения B-TERM
сущности I-TERM
) O
. O

# text =  результат первой модели – точность 77%
результат O
первой O
модели O
– O
точность B-TERM
77% B-TERM

# text =  Чтобы определить, какие ещё есть потенциальные классы, мы повели так называемое тематическое моделирование, используя несколько подходов: начиная от пробалистических моделей (латентное распределение Дирихле, ARTM) и всё те же нейронные сети (BERT).
Чтобы O
определить O
, O
какие O
ещё O
есть O
потенциальные O
классы O
, O
мы O
повели O
так O
называемое O
тематическое B-TERM
моделирование I-TERM
, O
используя O
несколько O
подходов O
: O
начиная O
от O
пробалистических B-TERM
моделей I-TERM
( O
латентное B-TERM
распределение I-TERM
Дирихле I-TERM
, O
ARTM B-TERM
) O
и O
всё O
те O
же O
нейронные B-TERM
сети I-TERM
( O
BERT B-TERM
) O
. O

# text =  Теперь нам нужно было использовать некоторые технические способы, чтобы сделать максимально высоким качество модели, которая на новых классах давала точность 72%.
Теперь O
нам O
нужно O
было O
использовать O
некоторые O
технические B-TERM
способы I-TERM
, O
чтобы O
сделать O
максимально O
высоким O
качество O
модели B-TERM
, O
которая O
на O
новых O
классах O
давала O
точность B-TERM
72 B-TERM
% I-TERM
. O

# text =  Второе, мы стандартно провели экстенсивный тюнинг гиперпараметров и изменили нашу метрику с точности на F1, чтобы ставить больше акцента на точность по каждому классу, так как общая точность предвзято относится к доминирующим классам.
Второе O
, O
мы O
стандартно O
провели O
экстенсивный B-TERM
тюнинг I-TERM
гиперпараметров I-TERM
и O
изменили O
нашу O
метрику O
с O
точности B-TERM
на O
F1 B-TERM
, O
чтобы O
ставить O
больше O
акцента O
на O
точность B-TERM
по O
каждому O
классу O
, O
так O
как O
общая O
точность B-TERM
предвзято O
относится O
к O
доминирующим O
классам O
. O

# text =   Мы провели экстенсивный тюнинг гиперпараметров и переключили нашу метрику с точности на F1.
Мы O
провели O
экстенсивный B-TERM
тюнинг I-TERM
гиперпараметров I-TERM
и O
переключили O
нашу O
метрику O
с O
точности B-TERM
на O
F1 B-TERM
. O

# text =  Изменение оптимизирующей метрики на F1 позволило алгоритму обучения дольше обучаться, так как почти на каждом этапе происходило улучшение по F1, когда метрика была точность, мы достигали плато гораздо быстрее.
Изменение O
оптимизирующей O
метрики O
на O
F1 B-TERM
позволило O
алгоритму O
обучения O
дольше O
обучаться O
, O
так O
как O
почти O
на O
каждом O
этапе O
происходило O
улучшение O
по O
F1 B-TERM
, O
когда O
метрика O
была O
точность B-TERM
, O
мы O
достигали O
плато O
гораздо O
быстрее O
. O

# text =  Изначально на этапе MVP (minimum viable product) мы применяли регулярные выражения для извлечения сущности.
Изначально O
на O
этапе O
MVP B-TERM
( O
minimum B-TERM
viable I-TERM
product I-TERM
) O
мы O
применяли B-TERM
регулярные B-TERM
выражения I-TERM
для B-TERM
извлечения B-TERM
сущности I-TERM
. O

# text =  Протестировав поведение модели на продовских данных, мы обнаружили, что точность извлечения была около 50%.
Протестировав O
поведение O
модели B-TERM
на O
продовских O
данных O
, O
мы O
обнаружили O
, O
что O
точность B-TERM
извлечения O
была O
около O
50 B-TERM
% I-TERM
. O

# text =  Мы поняли, что даже извлечение сущности зависит от контекста, и решили использовать BERT.
Мы O
поняли O
, O
что O
даже O
извлечение B-TERM
сущности I-TERM
зависит O
от O
контекста O
, O
и O
решили O
использовать B-TERM
BERT B-TERM
. O

# text =  На вход необходимо представить размеченные данные с маркировкой BIO (beginning, intermediate, O – пустота).
На O
вход O
необходимо O
представить O
размеченные O
данные O
с O
маркировкой B-TERM
BIO B-TERM
( O
beginning O
, O
intermediate O
, O
O O
– O
пустота O
) O
. O

# text =  Мы производили разметку 800 обращений на DataTurcks: Точность подхода BERT – 94% на этапе обучения, она валидирована на тестовых данных.
Мы O
производили O
разметку B-TERM
800 O
обращений O
на O
DataTurcks B-TERM
: O
Точность B-TERM
подхода B-TERM
BERT I-TERM
– O
94 B-TERM
% I-TERM
на O
этапе O
обучения O
, O
она O
валидирована O
на O
тестовых O
данных O
. O

# text =   Метод BERT демонстрирует точность 94% на этапе обучения, и эта точность проверена на тестовых данных.
Метод B-TERM
BERT I-TERM
демонстрирует B-TERM
точность B-TERM
94 B-TERM
% I-TERM
на O
этапе O
обучения O
, O
и O
эта O
точность B-TERM
проверена O
на O
тестовых O
данных O
. O

# text =  Это постобработка увеличила точность до 98%.
Это O
постобработка O
увеличила O
точность B-TERM
до O
98% B-TERM

# text =  В иностранной литературе можно встретить термин Continuous Learning (CL), который объединяет различные методы использования новых данных для поддержания эффективности моделей.
В O
иностранной O
литературе O
можно O
встретить O
термин O
Continuous B-TERM
Learning I-TERM
( O
CL B-TERM
) O
, O
который O
объединяет B-TERM
различные O
методы B-TERM
использования I-TERM
новых I-TERM
данных I-TERM
для I-TERM
поддержания I-TERM
эффективности I-TERM
моделей I-TERM
. O

# text =  Методы CL были положены в основу пайплайна переобучения.
Методы O
CL B-TERM
были O
положены O
в O
основу O
пайплайна O
переобучения O
. O
