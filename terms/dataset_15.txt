# text =  Типичным методом обучения без учителя является кластеризация, благодаря которому обучающая выборка разбивается на устойчивые группы или кластеры.
Типичным O
методом B-TERM
обучения I-TERM
без I-TERM
учителя I-TERM
является O
кластеризация B-TERM
, O
благодаря O
которому O
обучающая O
выборка B-TERM
разбивается B-TERM
на O
устойчивые B-TERM
группы I-TERM
или O
кластеры B-TERM
. O

# text =  Другой подход обучения без учителя для текстов называется тематическим моделированием (topic modeling), позволяющим выявить в неразмеченных текстах основные тематики.
Другой O
подход B-TERM
обучения I-TERM
без I-TERM
учителя I-TERM
для B-TERM
текстов B-TERM
называется O
тематическим B-TERM
моделированием I-TERM
( O
topic B-TERM
modeling I-TERM
) O
, O
позволяющим B-TERM
выявить B-TERM
в O
неразмеченных B-TERM
текстах I-TERM
основные B-TERM
тематики I-TERM
. O

# text =  Если отказываемся от методов unsupervised learning, то логично обратиться к методам обучения с учителем (supervised learning) и в частности к классификации.
Если O
отказываемся O
от O
методов O
unsupervised B-TERM
learning I-TERM
, O
то O
логично O
обратиться O
к O
методам B-TERM
обучения I-TERM
с I-TERM
учителем I-TERM
( O
supervised B-TERM
learning I-TERM
) O
и O
в O
частности O
к O
классификации B-TERM
. O

# text =  Результатом работы языковой модели являются эмбеддинги — это отображение из пространства слов в пространство векторов конкретной фиксированной длины, причем векторы, соответствующие близким по смыслу словам, будут расположены в новом пространстве рядом, а далекие по смыслу — далеко.
Результатом O
работы O
языковой B-TERM
модели I-TERM
являются O
эмбеддинги B-TERM
— O
это O
отображение O
из O
пространства O
слов B-TERM
в O
пространство O
векторов O
конкретной O
фиксированной O
длины O
, O
причем O
векторы O
, O
соответствующие O
близким O
по O
смыслу B-TERM
словам I-TERM
, O
будут O
расположены O
в O
новом O
пространстве O
рядом O
, O
а O
далекие O
по O
смыслу O
— O
далеко O
. O

# text =  При использовании TF-IDF (например, вот) подхода с фильтром по частотам и логистической регрессии уже можно получить прекрасные результаты: изначально в краулер отправлялись очень разные тексты, и модель прекрасно справляется.
При O
использовании B-TERM
TF B-TERM
- I-TERM
IDF I-TERM
( O
например O
, O
вот O
) O
подхода O
с O
фильтром O
по O
частотам O
и O
логистической B-TERM
регрессии I-TERM
уже O
можно O
получить O
прекрасные O
результаты O
: O
изначально O
в O
краулер O
отправлялись O
очень O
разные O
тексты O
, O
и O
модель O
прекрасно O
справляется O
. O

# text =   Используя TF-IDF с фильтром по частотам и логистической регрессией, уже можно достичь отличных результатов.
Используя B-Metric_isAppliedTo_Method
TF B-TERM
- I-TERM
IDF I-TERM
с O
фильтром O
по O
частотам O
и O
логистической B-TERM
регрессией I-TERM
, O
уже O
можно O
достичь O
отличных O
результатов O
. O

# text =  Для каждой из популяций рассчитаем word2vec расстояние до центра положительной обучающей выборки.
Для O
каждой O
из O
популяций O
рассчитаем O
word2vec B-TERM
расстояние O
до O
центра O
положительной O
обучающей O
выборки B-TERM
. O

# text =  Распределения можно разделить, и для оценки расстояния между распределениями в первую очередь логично обратиться к Дивергенции Кульбака-Лейблера (ДКЛ).
Распределения O
можно O
разделить O
, O
и O
для B-TERM
оценки B-TERM
расстояния I-TERM
между O
распределениями O
в O
первую O
очередь O
логично O
обратиться O
к O
Дивергенции B-TERM
Кульбака I-TERM
- I-TERM
Лейблера I-TERM
( O
ДКЛ B-TERM
) O
. O
