{
  "articles": [
    {
      "text_804515": [
        {
          "text": "Хабр, привет! Меня зовут Антон Разжигаев, я аспирант Сколтеха и участник научной группы Fusion Brain в институте AIRI.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Хабр",
              "start_pos": 0
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Антон Разжигаев",
              "start_pos": 25
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Сколтеха",
          "start_pos": 53
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "AIRI",
          "start_pos": 113
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "Fusion Brain",
          "start_pos": 88
            }
          ],
          "relations": []
        },
        {
          "text": "С момента выхода первой статьи «Attention is All You Need» я с жадностью и любопытством, присущими любому исследователю, пытаюсь углубиться во все особенности и свойства моделей на базе архитектуры трансформер. Но, если честно, я до сих пор не понимаю, как они работают и почему так хорошо обучаются. Очень хочу разобраться, в чём же причина такой эффективности этих моделей, и есть ли предел их возможностей?",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Attention is All You Need",
          "start_pos": 32
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей на базе архитектуры трансформер",
          "start_pos": 170
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
          "start_pos": 170
            }
          ],
          "relations": []
        },
        {
          "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеров",
          "start_pos": 16
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "конференции EACL 2024",
          "start_pos": 110
            },
            {
              "index": 0,
              "class": "InfoResource",
              "value": "The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models",
          "start_pos": 164
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддингов",
          "start_pos": 317
            },
            {
              "index": 0,
              "class": "Object",
              "value": "активаций",
          "start_pos": 330
            },
            {
              "index": 0,
              "class": "Model",
              "value": "языковых моделей",
          "start_pos": 401
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LM",
          "start_pos": 419
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "языковых моделей",
                "index": 0,
            "start_pos": 401
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "LM",
                "index": 0,
            "start_pos": 419
              }
            }
          ]
        },
        {
          "text": "Итак, приступим!",
          "terms": [],
          "relations": []
        },
        {
          "text": "Начнём с рассказа о данных — это нужно для того, чтобы было проще понять, что мы сделали и что обнаружили. Т.к. нас интересовало пространство контекстуализированных эмбеддингов (в т.ч. промежуточных), надо было их где-то добыть.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "контекстуализированных эмбеддингов",
          "start_pos": 142
            }
          ],
          "relations": []
        },
        {
          "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "enwik8",
          "start_pos": 9
            },
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Википедии",
          "start_pos": 45
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английском языке",
          "start_pos": 58
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
          "start_pos": 115
            },
            {
              "index": 0,
              "class": "Object",
              "value": "токена",
          "start_pos": 173
            },
            {
              "index": 0,
              "class": "Object",
              "value": "«пространство эмбеддингов»",
          "start_pos": 215
            },
            {
              "index": 0,
              "class": "Object",
              "value": "многомерное облако точек",
          "start_pos": 264
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Object",
                "value": "«пространство эмбеддингов»",
                "index": 0,
            "start_pos": 215
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Object",
                "value": "многомерное облако точек",
                "index": 0,
            "start_pos": 264
              }
            },
            {
              "term1": {
                "class": "InfoResource",
                "value": "Википедии",
                "index": 0,
            "start_pos": 45
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском языке",
                "index": 0,
            "start_pos": 58
              }
            },
            {
              "term1": {
                "class": "Dataset",
                "value": "enwik8",
                "index": 0,
            "start_pos": 9
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском языке",
                "index": 0,
            "start_pos": 58
              }
            }
          ]
        },
        {
          "text": "Чтобы исключить зависимость наблюдений от выбранного датасета, мы повторили эксперименты на случайных последовательностях токенов, и все выводы повторились. Поэтому в дальнейшем не буду акцентировать на этом внимание, а лучше сразу перейду к результатам.",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "датасета",
          "start_pos": 53
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "эксперименты",
          "start_pos": 75
            },
            {
              "index": 0,
              "class": "Object",
              "value": "токенов",
          "start_pos": 122
            }
          ],
          "relations": []
        },
        {
          "text": "Один из самых главных вопросов, которые мы себе задали в процессе исследования — а какая вообще форма у этих облаков точек? Визуализировать их сложно — пространство эмбеддингов очень многомерно, — а методы снижения размерности тут не сильно помогают. Поэтому мы решили использовать анизотропию в качестве нашего «микроскопа». Анизотропия — это мера, показывающая насколько облако точек вытянуто, насколько оно неоднородно. Чем выше это значение, тем сильнее вытягивается пространство.",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "исследования",
          "start_pos": 66
            },
            {
              "index": 0,
              "class": "Task",
              "value": "Визуализировать",
          "start_pos": 124
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддингов",
          "start_pos": 165
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методы снижения размерности",
          "start_pos": 199
            },
            {
              "index": 0,
              "class": "Method",
              "value": "анизотропию",
          "start_pos": 282
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "Анизотропия",
          "start_pos": 326
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "методы снижения размерности",
                "index": 0,
            "start_pos": 199
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "Визуализировать",
                "index": 0,
          "start_pos": 124
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "анизотропию",
                "index": 0,
          "start_pos": 282
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "Визуализировать",
                "index": 0,
          "start_pos": 124
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "методы снижения размерности",
                "index": 0,
            "start_pos": 199
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "эмбеддингов",
                "index": 0,
            "start_pos": 165
              }
            }
          ]
        },
        {
          "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Representation Degeneration Problem in Training Natural Language Generation Models",
          "start_pos": 42
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддинги",
          "start_pos": 131
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеров-энкодеров",
          "start_pos": 142
            },
            {
              "index": 0,
              "class": "Object",
              "value": "текстовыми репрезентациями",
          "start_pos": 218
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддинги",
          "start_pos": 442
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеров-энкодеров",
          "start_pos": 453
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Bert",
          "start_pos": 478
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoBERTa",
          "start_pos": 484
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Albert",
          "start_pos": 494
            },
            {
              "index": 0,
              "class": "Model",
              "value": "декодерами",
          "start_pos": 537
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT",
          "start_pos": 549
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Llama",
          "start_pos": 554
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Mistral",
          "start_pos": 561
            },
            {
              "index": 0,
              "class": "Method",
              "value": "центрирования",
          "start_pos": 626
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методов на базе сингулярных",
          "start_pos": 684
            },
            {
              "index": 0,
              "class": "Model",
              "value": "языковых моделей",
          "start_pos": 749
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропия",
          "start_pos": 766
            },
            {
              "index": 0,
              "class": "Value",
              "value": "практически равна 1",
          "start_pos": 778
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
          "start_pos": 908
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "анизотропия",
                "index": 0,
          "start_pos": 766
              },
              "predicate": "hasValue",
              "term2": {
                "class": "Value",
                "value": "практически равна 1",
                "index": 0,
          "start_pos": 778
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "анизотропия",
                "index": 0,
          "start_pos": 766
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "декодерами",
                "index": 0,
          "start_pos": 537
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "центрирования",
                "index": 0,
          "start_pos": 626
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "декодерами",
                "index": 0,
          "start_pos": 537
              }
            }
          ]
        },
        {
          "text": "Откуда берётся эта неоднородность пространства репрезентаций в декодерах, мы пока не знаем, но предполагаем, что это связано с процессом их обучения, задачей предсказания следующего токена и треугольной маской внимания. Это одна из исследовательских задач, которая сейчас стоит перед нами.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "декодерах",
          "start_pos": 63
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучения",
          "start_pos": 140
            },
            {
              "index": 0,
              "class": "Task",
              "value": "предсказания следующего токена",
          "start_pos": 158
            },
            {
              "index": 0,
              "class": "Method",
              "value": "треугольной маской внимания",
          "start_pos": 192
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "декодерах",
                "index": 0,
            "start_pos": 63
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "предсказания следующего токена",
                "index": 0,
            "start_pos": 158
              }
            }
          ]
        },
        {
          "text": "Если посмотреть на профиль анизотропии по слоям, то становится видно, что в начале и в конце декодеров эмбеддинги гораздо более изотропны, а экстремально высокая анизотропия наблюдается только в середине, где и должен происходить весь мыслительный процесс.",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропии",
          "start_pos": 27
            },
            {
              "index": 0,
              "class": "Model",
              "value": "декодеров",
          "start_pos": 93
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддинги",
          "start_pos": 103
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропия",
          "start_pos": 162
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "анизотропии",
                "index": 0,
          "start_pos": 27
              },
              "predicate": "isUsedIn",
              "term2": {
                "class": "Model",
                "value": "декодеров",
                "index": 0,
          "start_pos": 93
              }
            }
            ]
        },
        {
          "text": "Мы проследили за тем, как меняется анизотропия от чекпоинта к чекпоинту по мере обучения моделей (мы взяли все модели с промежуточными весами из того, что было опубликовано на тот момент). Оказалось, что все модели класса трансформер-декодер постепенно сходятся к одной и той же форме пространства и одному и тому же куполообразному профилю анизотропии.",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропия",
          "start_pos": 35
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
          "start_pos": 89
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
          "start_pos": 111
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
          "start_pos": 208
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформер-декодер",
          "start_pos": 222
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропии",
          "start_pos": 341
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "анизотропия",
                "index": 0,
          "start_pos": 35
              },
              "predicate": "isUsedIn",
              "term2": {
                "class": "Model",
                "value": "моделей",
                "index": 0,
          "start_pos": 89
              }
            }
            ]
        },
        {
          "text": "Следующий наш «микроскоп» для наблюдения за активациями — внутренняя размерность. Это довольно красивое математическое понятие, описывающее «сложность» фигуры (многообразия или манифолда), на котором располагаются точки в многомерном пространстве.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "активациями",
          "start_pos": 44
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "внутренняя размерность",
          "start_pos": 58
            }
          ],
          "relations": []
        },
        {
          "text": "Чтобы было понятнее, рассмотрим трёхмерную фигуру в виде ленты, свёрнутой в спираль (см. картинку ниже). Если мы приблизимся к какому-либо её участку, то обнаружим, что в малой окрестности точки будто бы лежат на плоскости. Следовательно, локальная внутренняя размерность тут равна двум.",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "внутренняя размерность",
          "start_pos": 249
            }
          ],
          "relations": []
        },
        {
          "text": "Самое главное, что внутреннюю размерность довольно легко оценить, так как она сильно связана со скоростью роста «объёма» многомерного шара (количества точек данных, попадающих внутрь шара) по мере увеличения радиуса. Измерение зависимости количества точек от радиуса позволяет определить внутреннюю размерность в локальной области облака данных.",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "внутреннюю размерность",
          "start_pos": 19
            }
          ],
          "relations": []
        },
        {
          "text": "Итак, что же мы обнаружили? Во-первых, размерность довольно низкая, но это не новость, т.к. это было обнаружено и до нас. Во-вторых, — и это гораздо интереснее — эта размерность изменяется одинаково для всех моделей по мере обучения! Этот процесс состоит из двух фаз — сначала рост, а затем падение (см. график).",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "размерность",
          "start_pos": 39
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "размерность",
          "start_pos": 166
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
          "start_pos": 208
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучения",
          "start_pos": 224
            }
          ],
          "relations": []
        },
        {
          "text": "Похоже, что первая часть обучения переводит фичи в более высокие измерения, чтобы «запомнить» как можно больше информации, а во второй фазе — фичи начинают сжиматься, позволяя выявлять больше закономерностей, усиливая обобщающие способности модели.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "обучения",
          "start_pos": 25
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
          "start_pos": 241
            }
          ],
          "relations": []
        },
        {
          "text": "Ещё раз — у всех LLM во время обучения присутствуют две фазы: инфляция эмбеддингов и их последующая компрессия.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
          "start_pos": 17
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучения",
          "start_pos": 30
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддингов",
          "start_pos": 71
            },
            {
              "index": 0,
              "class": "Method",
              "value": "инфляция",
          "start_pos": 62
            },
            {
              "index": 0,
              "class": "Method",
              "value": "компрессия",
          "start_pos": 100
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "инфляция",
                "index": 0,
          "start_pos": 62
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучения",
                "index": 0,
          "start_pos": 30
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "компрессия",
                "index": 0,
          "start_pos": 100
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучения",
                "index": 0,
          "start_pos": 30
              }
            }
            ]
        },
        {
          "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "обучения",
          "start_pos": 70
            },
            {
              "index": 0,
              "class": "Model",
              "value": "языковых моделей",
          "start_pos": 79
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеры",
          "start_pos": 143
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддинги",
          "start_pos": 190
            },
            {
              "index": 0,
              "class": "Method",
              "value": "компрессии",
          "start_pos": 217
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
          "start_pos": 353
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "компрессии",
                "index": 0,
          "start_pos": 217
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "эмбеддинги",
                "index": 0,
          "start_pos": 190
              }
            }
          ]
        },
        {
          "text": "Также мы обнаружили, что незадолго до взрывов лосса во время обучения (больная тема всех, кто учит LLM) внутренняя размерность сильно подрастает. Возможно, у нас получится предсказывать взрывы лосса и не тратить вычислительные ресурсы впустую, или вообще победить эти нестабильности, поняв их природу.",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "учит"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "внутренняя размерность"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "внутренняя размерность",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
        {
          "term1": {
            "class": "Method",
            "value": "компрессии",
            "start_pos": 217
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "трансформеры",
            "start_pos": 143
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "компрессии",
            "start_pos": 217
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "языковых моделей",
            "start_pos": 79
          }
        }
          ]
        },
        {
          "text": "Хотя кого я обманываю, всё это нужно только ради удовлетворения своего любопытства! ",
          "terms": [],
          "relations": []
        },
        {
          "text": "Подписывайтесь на каналы авторов в телеграме  AbstractDL, CompleteAI, Dendi Math&AI, Ivan Oseledets. В работе также принимали участие коллеги из AIRI, Сбера, Сколтеха, МГУ, ВШЭ и Самарского университета.",
          "terms": [
            {
              "index": 0,
              "class": "Organization",
              "value": "AIRI"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Сбера"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Сколтеха"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "МГУ"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "ВШЭ"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Самарского университета"
            }
          ],
          "relations": []
        }
      ],
      "text_803945": [
        {
          "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "большие языковые модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "написание кода"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "создание контента"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "перевод текстов"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "большие языковые модели",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "написание кода",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "большие языковые модели",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "создание контента",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "большие языковые модели",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "перевод текстов",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "большие языковые модели",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Кто бы подумал, что искусственный интеллект кусается? На деле, конечно, дело не в физическом нападении, а в уязвимостях, которые могут быть использованы злоумышленниками. Большие языковые модели действительно могут попасть под угрозу, и влияние таких событий может оказаться далеко не виртуальным.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "Большие языковые модели"
            }
          ],
          "relations": []
        },
        {
          "text": "Меня зовут Дарья Лютова, я data scientist в ЦАД ВАВТ, также я учусь в магистратуре AI Talent Hub ИТМО и интересуюсь вопросами обучения и безопасности языковых моделей. В этом посте, вместе с вами, хочу пойти дальше простого обсуждения существования уязвимостей в LLM и предлагаю вникнуть в тему проблем безопасности, касающуюся больших языковых моделей, выявить слабые места и прийти к пониманию методов их укрепления. Очень надеюсь, что эта информация поможет тем, кто преследует цель не только достичь новых высот в области AI, но и удостовериться, что их достижения надежны и устойчивы к киберугрозам.",
          "terms": [
            {
              "index": 0,
              "class": "Person",
              "value": "Дарья Лютова"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "ЦАД ВАВТ"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "AI Talent Hub ИТМО"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "языковых моделей"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "больших языковых моделей"
            }
          ],
          "relations": []
        },
        {
          "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "Большие языковые модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Large Language Models"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "нейронные сети"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT4"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Mistral 7B OpenChat"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Claude 3 Opus"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "Large Language Models",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "Большие языковые модели",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "Large Language Models",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "Большие языковые модели",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT4",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Mistral 7B OpenChat",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Claude 3 Opus",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Про LLM знают почти все, но не очень многим известно, что эти модели могут быть уязвимы, причем данном случае речь идет не о сбоях в работе модели или выдаче неверного ответа, речь о целенаправленных атаках злоумышленников с целью получения конфиденциальной информации, утечкам данных (много и интересно про уязвимости в статье), выдачи вредоносного контента, например, рекомендации посетить нежелательный сайт, а также манипуляции и дезинформации. В LLM могут быть заложены предвзятости, обусловленные предвзятостью в наборах данных для обучения, а это может привести к дискриминационной, ошибочной или предвзятой генерации контента. Большие языковые модели могут стать мишенью для специфических программных атак, нацеленных на эксплуатацию уязвимостей в алгоритмах или инфраструктуре, что может нарушить их функционирование или повлиять на качество генерируемого контента.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Большие языковые модели"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "генерации контента",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Т.к. сейчас LLM используют очень многие, важно понимать эти уязвимости и принимать меры для обеспечения безопасности моделей и данных, с которыми они работают.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обеспечения безопасности"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей"
            }
          ],
          "relations": []
        },
        {
          "text": "По какой причине в таком мощном инструменте могут возникать уязвимости, некоторые из которых могут быть использованы злоумышленниками? Это происходит из-за сложности и размера самих моделей, которые могут стать мишенью для специально разработанных атак. Кроме того, часто данные, на которых обучаются эти модели, могут быть неочищенными и содержать предвзятость, которая затем отражается в работе LLM. Недостаточное тестирование моделей на безопасность также может стать причиной возникновения уязвимостей.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            }
          ],
          "relations": []
        },
        {
          "text": "Авторы статьи Jailbroken: How does llm safety training fail? говорят о двух, на их взгляд, основных причинах того, что взлом моделей возможен.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Jailbroken: How does llm safety training fail"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей"
            }
          ],
          "relations": []
        },
        {
          "text": "Конкурирующие цели (competing objectives) возникают в ситуациях, когда задачи предварительного обучения и следования инструкциям модели противоречат задаче безопасности.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            }
          ],
          "relations": []
        },
        {
          "text": "Например, мы конструируем промпт (запрос к модели) так, чтобы модель сначала ответила на какой-то простой и безопасный вопрос или выполнила какую-то простую инструкцию. Это базируется на том, что модели наказывают за отказ от безвредных инструкций. Если следующей частью промпта идет какой-то небезопасный вопрос, то модель скорее всего ответит и на него, поскольку отказ после старта генерации в предварительном обучении маловероятен и основная задача предварительного обучения заключается в наказании за отказ. В результате модель продолжает отвечать уже на небезопасный запрос.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "промпт"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "запрос к модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "промпта"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "генерации"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "наказании за отказ"
            }
          ],
          "relations": []
        },
        {
          "text": "Несоответствующее обобщение (mismatched generalization) возникает, когда входные данные не попали в корпус для обучения модели на безопасность, но находятся в рамках более широкого и разнообразного набора данных предварительного обучения модели на разные задачи. Такое несоответствие может быть использовано для взлома путем конструирования запросов, на которых предварительное обучение и следование инструкциям обобщаются, но обучение на безопасность не работает. В таких случаях модель отвечает, но без учёта безопасности.",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "корпус"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучения модели на безопасность"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            }
          ],
          "relations": []
        },
        {
          "text": "В данном случае нам достаточно перевести небезопасный запрос на один из малоресурсных языков, и, с большой вероятностью успеха, модель выдаст нам вредоносный ответ.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "небезопасный запрос"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсных языков"
            }
            ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Lang",
                "value": "малоресурсных языков",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Комбинация этих двух атак также может быть успешной. Но если для первой угрозы порой надо поломать голову и придумать что-то, что заставить модель отвечать нам положительно, то вторая атака порой требует только качественного переводчика на один из малоресурсных языков.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "атак"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "первой угрозы"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "вторая атака"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсных языков"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "первой угрозы",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Task",
                "value": "атак",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "вторая атака",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Task",
                "value": "атак",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Lang",
                "value": "малоресурсных языков",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Второй тип атаки хочется рассмотреть подробнее.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "Второй тип атаки"
            }
            ],
          "relations": []
        },
        {
          "text": "Для обеспечения безопасности и защиты от злоупотреблений, компании разработчики языковых моделей, такие как OpenAI и Anthropic, применяют подходы, основанные на обучении с подкреплением от человека (RLHF), и стратегии \"red team\". В рамках RLHF модели обучаются на данных, связанных с безопасностью, стремясь максимизировать вознаграждения, аналогичные человеческому суждению о безопасном контенте. Задача \"red team\" включает в себя идентификацию и устранение уязвимостей защиты через ряд методов, включая переобучение модели и фильтрацию данных, для предотвращения генерации вредоносного контента перед выпуском моделей. \"Red team\" играют важную роль в безопасности больших языковых моделей, проводя симулированные атаки и тестирование на проникновение для выявления уязвимостей и разработки мер по устранению рисков безопасности. Их участие помогает повысить уровень защиты и обеспечить безопасное использование LLM.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "обеспечения безопасности"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "защиты от злоупотреблений"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "OpenAI"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Anthropic"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "обучении с подкреплением от человека"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RLHF"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "\"red team\""
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RLHF"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "\"red team\""
            },
            {
              "index": 0,
              "class": "Task",
              "value": "идентификацию"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "устранение уязвимостей защиты"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "переобучение модели"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "фильтрацию данных"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "предотвращения генерации вредоносного контента"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "\"Red team\""
            },
            {
              "index": 0,
              "class": "Method",
              "value": "симулированные атаки"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "тестирование на проникновение"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "выявления уязвимостей"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "устранению рисков безопасности"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "повысить уровень защиты"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обеспечить безопасное использование"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "обучении с подкреплением от человека",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обеспечения безопасности",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "переобучение модели",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "идентификацию",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "переобучение модели",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранение уязвимостей защиты",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "переобучение модели",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "предотвращения генерации вредоносного контента",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "фильтрацию данных",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "идентификацию",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "фильтрацию данных",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранение уязвимостей защиты",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "фильтрацию данных",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "предотвращения генерации вредоносного контента",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "симулированные атаки",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "выявления уязвимостей",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "симулированные атаки",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранению рисков безопасности",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "симулированные атаки",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "повысить уровень защиты",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "тестирование на проникновение",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "выявления уязвимостей",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "тестирование на проникновение",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранению рисков безопасности",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "тестирование на проникновение",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "повысить уровень защиты",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Исследования показали, что атаки, основанные на использовании нетрадиционных или неанглийских языков, включая обфускацию с помощью кодировки base64, азбуки Морзе и специальных шифров, могут успешно обходить защитные механизмы LLM. Такие методы позволяют скрыто ввести вредоносные подсказки или запросы, не будучи распознанными системами безопасности.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "атаки"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "нетрадиционных или неанглийских языков"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "вредоносные подсказки или запросы"
            }
          ],
          "relations": []
        },
        {
          "text": "Но уже есть готовые инструменты проверки или защиты LLM от таких перекодированных взломов. Например, в сканере уязвимостей для больших языковых моделей Garak (https://github.com/leondz/garak) существует большое количество проб на перекодированные запросы, включая запросы на юникоде, азбуке Морзе, Вase(16, 32, 64, 2048), ROT13, NATO phonetic alphabet и многих других. Такие проверки не требуют носителя языка и достаточно просто проверяются кодом.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "больших языковых моделей Garak"
            }
          ],
          "relations": []
        },
        {
          "text": "Это подчеркивает сложности в обеспечении безопасности AI, особенно против замаскированных или кодированных атак и демонстрирует, что естественный язык может представлять более сложную задачу для безопасности по сравнению с более простыми или строго формализованными кодами вроде азбуки Морзе или base64. Важность этих находок заключается в необходимости разработки методов защиты, способных эффективно распознавать и противостоять разнообразным формам ввода, включая маскировку под безобидный или нетипичный контент.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "обеспечении безопасности AI"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "разработки методов защиты"
            }
            ],
          "relations": []
        },
        {
          "text": "При разработке решений искусственного интеллекта на разных языках критически важно иметь доступ к данным на всех языках. Согласно \"Этнологу\", в мире используется 7164 языка, при этом китайский, испанский и английский являются языками с наибольшим числом носителей. Тем не менее, только около 20 языков обладают обширными текстовыми корпусами, с английским на первом месте. Большинство азиатских и африканских языков страдают от нехватки данных, что делает их малоресурсными и затрудняет разработку ИИ-решений. Языки, включая суахили и хинди, меньше представлены в интернете, что усложняет создание решений в области обработки естественного языка для них. Наличие большого объема текстовых данных и специализированных ресурсов, таких как семантические базы данных, является ключевым для разработки эффективных языковых решений. Подходы к преодолению проблем малоресурсных языков включают увеличение данных, мета-трансферное обучение и межъязыковые аннотации, что позволяет улучшить работу ИИ на разных языках и расширить его возможности для малоресурсных языков.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "\"Этнологу\""
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "корпусами"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "китайский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "испанский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английским"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "азиатских и африканских языков"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "разработку ИИ-решений"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "суахили"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хинди"
            },
            {
              "index": 0,
              "class": "Science",
              "value": "обработки естественного языка"
            },
            {
              "index": 0,
              "class": "InfoResource",
              "value": "семантические базы данных"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсных языков"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "увеличение данных"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "мета-трансферное обучение"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "межъязыковые аннотации"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсных языков"
            }
          ],
          "relations": []
        },
        {
          "text": "Таким образом, существует большое количество методов, чтобы обеспечить носителям малоресурсных языков работу с большими языковыми моделями. При этом в области безопасности ИИ существует лингвистическим неравенство, т.к. в «red team» не включены носители малоресурсных языков.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "большими языковыми моделями"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "«red team»"
            }
          ],
          "relations": []
        },
        {
          "text": "Получается, что модель на малоресурсных языках обучена, но почти не защищена от вредоносных запросов на этих языках!",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            }
          ],
          "relations": []
        },
        {
          "text": "В этой области довольно иллюстративным примером является эксперимент объединенной команды авторов из Брауновского университета (Zheng-Xin Yong, CristinaMenghini, and Stephen H. Bach.), которые разделили языки на 3 категории:",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "эксперимент"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Брауновского университета"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Zheng-Xin Yong"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "CristinaMenghini"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Stephen H. Bach."
            },
            {
              "index": 0,
              "class": "Object",
              "value": "языки"
            }
          ],
          "relations": []
        },
        {
          "text": "Низкоресурсные языки - это такие языки, которые почти не имеют доступа к данным для обучения ИИ. Сюда входят, например, зулу, шотландский гэльский, хмонг, и гуарани. Они представляют большую часть мировых языков (94%) и имеют около 1,2 миллиарда пользователей.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "Низкоресурсные языки"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "зулу"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "шотландский гэльский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хмонг"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "гуарани"
            }
          ],
          "relations": []
        },
        {
          "text": "Среднересурсные языки - языки с некоторым количеством доступных данных: украинский, бенгальский, тайский и иврит. Они занимают 4,5% от языков мира, насчитывая 1,8 миллиарда пользователей.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "Среднересурсные языки"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "украинский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "бенгальский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "тайский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "иврит"
            }
          ],
          "relations": []
        },
        {
          "text": "Высокоресурсные языки имеют обширную базу данных, как неразмеченных, так и c метками: упрощенный мандаринский, современный арабский, итальянский, хинди и английский. Эти языки составляют 1,5% от языков мира, но на них приходится 4,7 миллиарда пользователей.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "Высокоресурсные языки"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "упрощенный мандаринский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "современный арабский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "итальянский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хинди"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английский"
            }
          ],
          "relations": []
        },
        {
          "text": "Авторы перевели каждую опасную инструкцию из набора данных AdvBench Harmful Behaviors на 12 языков трех разных уровней ресурсов, выбрав языки из различных географических мест и языковых групп, чтобы результаты были универсальными. Непереведенные запросы на английском были добавлены как бейзлайн для сравнения.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "опасную инструкцию"
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "AdvBench Harmful Behaviors"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английском"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Dataset",
                "value": "AdvBench Harmful Behaviors",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Чтобы оценить степень угрозы атак на основе перевода, авторы сравнили их с наиболее успешными методами взлома: AIM, base64, инъекция префикса и подавление отказа.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "оценить степень угрозы атак"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "AIM"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "base64"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "инъекция префикса"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "подавление отказа"
            }
          ],
          "relations": []
        },
        {
          "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "опасные запросы"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсные языки"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "зулу"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "шотландский гэльский"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-4"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английском языке"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хмонг"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "гуарани"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "менее 1%"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсные языки"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-4"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английский"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "79%"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "зулу",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "шотландский гэльский",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "хмонг",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "гуарани",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском языке",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английский",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "В отличие от этого, языки со средним и высоким уровнем ресурсов оказались надежнее защищенными, с индивидуальным процентом успеха атак менее 15%. Была замечена разница в успехе атак в зависимости от языка, например, хинди, тайский и бенгальский показывают более высокие показатели.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "языки со средним и высоким уровнем ресурсов"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хинди"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "тайский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "бенгальский"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "индивидуальным процентом успеха атак"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "менее 15%"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "индивидуальным процентом успеха атак",
                "index": 0
              },
              "predicate": "hasValue",
              "term2": {
                "class": "Value",
                "value": "менее 15%",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Небезопасные запросы из набора данных AdvBench были классифицированы по 16 темам, и был проанализирован успех атак в зависимости от тематики и уровня ресурсов языков. С переводом на малоресурсные языки, обход средств безопасности оказался более успешным по всем темам, за исключением материалов, связанных с сексуальным насилием над детьми, где атаки на мало- и среднересурсных языках показали одинаковый успех из-за успешного обхода на тайском языке. Три темы с самым высоким процентом успешных атак через перевод на малоресурсные языки были: терроризм, финансовая манипуляция и дезинформация. ",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "AdvBench"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "темам"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "атак"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "тематики"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "уровня ресурсов языков"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "темам"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "темы"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "материалов"
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "классифицированы"
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "проанализирован"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "тайском языке"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "атак"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Dataset",
                "value": "AdvBench",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "тайском языке",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Недостаточное внимание к малоресурсным языкам может также увеличить риски смешанных атак. Вероятность встретить вредоносный контент на языках с низким уровнем доступности примерно в три раза выше, чем на языках с высоким уровнем доступности, как в случае с ChatGPT, так и с GPT-4. В преднамеренном сценарии многоязычные подсказки могут усугубить негативное влияние вредоносных инструкций, при этом частота появления небезопасных сообщений поразительно высока: 80,92 % для ChatGPT и 40,71 % для GPT-4 (Multilingual jailbreak challenges in large language models).",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "ChatGPT"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-4"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "Multilingual jailbreak challenges in large language models"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "частота появления небезопасных сообщений"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "80,92 %"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "40,71 %"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "Multilingual jailbreak challenges in large language models",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "ChatGPT",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "Multilingual jailbreak challenges in large language models",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "Multilingual jailbreak challenges in large language models",
                "index": 0
              },
              "predicate": "hasValue",
              "term2": {
                "class": "Value",
                "value": "80,92 %",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "Multilingual jailbreak challenges in large language models",
                "index": 0
              },
              "predicate": "hasValue",
              "term2": {
                "class": "Value",
                "value": "40,71 %",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Малоресурсные языки представляют собой особый вызов для механизмов безопасности GPT-4, позволяя обойти их с высокой долей успеха, что связано со случаями не соответствующего обобщения, когда обучение безопасности не обобщается на языковую область с низкими ресурсами, для которой существуют возможности LLM. Это подчеркивает необходимость улучшения моделей обнаружения и перевода для языков с низким уровнем ресурсов, чтобы повысить безопасность и эффективность защиты от вредоносных действий.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-4"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучение безопасности"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "улучшения моделей обнаружения и перевода"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "повысить безопасность"
            }
          ],
          "relations": []
        },
        {
          "text": "Разработчики приложений, интегрирующих такие мощные модели, должны быть настороже и принимать активные меры, чтобы устранить возможные уязвимости. Одним из путей может быть использование чат-ботов с функциями предварительной проверки и ограничения запросов, особенно для малоресурсных языков. Важно не просто ограничиваться контролем русско -и англоязычного контента, но и обеспечивать тщательный мониторинг вводимых данных на всех поддерживаемых языках.",
          "terms": [
            {
              "index": 0,
              "class": "Application",
              "value": "чат-ботов"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "предварительной проверки и ограничения запросов"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "устранить возможные уязвимости"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "мониторинг вводимых данных"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "предварительной проверки и ограничения запросов",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранить возможные уязвимости",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Включение дополнительных слов проверки запросов на малоресурсных языках может предотвратить обработку вредоносных инструкций. Подходы вроде применения инструментов типа Lakera Guard могут служить эффективным средством предотвращения выполнения вредоносных запросов. Реализуя механизмы перехвата опасных команд еще до активации модели, можно существенно повысить уровень цифровой безопасности.",
          "terms": [
            {
              "index": 0,
              "class": "Application",
              "value": "Lakera Guard"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "предотвращения выполнения вредоносных запросов"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "перехвата опасных команд"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "повысить уровень цифровой безопасности"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Application",
                "value": "Lakera Guard",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "предотвращения выполнения вредоносных запросов",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Application",
                "value": "Lakera Guard",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "перехвата опасных команд",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Application",
                "value": "Lakera Guard",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "повысить уровень цифровой безопасности",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Есть еще более простой выход из ситуации: если приложение ориентировано на использование только определенных языков – например, русского и английского – устанавливается ясная и строгая директива модели: обрабатывать запросы исключительно на этих языках. Можно внести четкое указание в системный промпт: “Эта модель предназначена для работы только с русским и английским языками. Ты не можешь принимать запросы ни на каких других языка, кроме русского и английского”. Такой подход использует базовый навык моделей следовать предоставленным инструкциям и создает барьер для запросов, выполненных на любых других языках. Эта мера, простая в реализации, может стать первой линией защиты от несанкционированных манипуляций и укрепит безопасность приложений, в которых используются большие языковые модели.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "русского"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английского"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "инструкциям"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "системный промпт"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "большие языковые модели"
            }
          ],
          "relations": []
        },
        {
          "text": "С другой стороны, стоит ожидать и надеяться, что компании-разработчики LLM, такие как OpenAI, будут вкладывать существенные ресурсы в совершенствование тестирования и устранение недочетов, связанных с малоресурсными языками. Уточнение и совершенствование моделей перевода и обнаружения языков с ограниченными данными является ключевой задачей, поскольку это напрямую влияет на безопасность AI-платформ в целом. Для сохранения информативности и полезности таких моделей, данных на малоресурсных языках нельзя исключать из обучения. Вместо этого следует настраивать системы таким образом, чтобы с одной стороны обеспечивалась должная обработка данных на этих языках, а с другой — предотвращались потенциальные угрозы безопасности до того, как механизмы безопасности соответствуют необходимым стандартам. Этот процесс является динамичным и требует постоянного внимания, так как только через неустанное мониторинг и обновление систем мы можем гарантировать, что искусственный интеллект останется надежным и безопасным для всех пользователей, независимо от популярности их языка.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "OpenAI"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "Уточнение и совершенствование"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей перевода и обнаружения языков"
            },
            {
              "index": 0,
              "class": "Application",
              "value": "AI-платформ"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            }
          ],
          "relations": []
        },
        {
          "text": "Безопасность LLM является сложной и многогранной проблемой, требующей комплексного подхода и сотрудничества различных сторон – от разработчиков и исследователей до пользователей и законодателей. Только совместными усилиями можно обеспечить безопасное и эффективное использование больших языковых моделей в различных областях и предотвратить возможные негативные последствия и угрозы для общества.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "больших языковых моделей"
            }
          ],
          "relations": []
        },
        {
          "text": "Обфускация - это процесс изменения промпта (входных данных) таким образом, чтобы он выглядел непонятным или запутанным, но при этом сохранял свою семантику (смысл). Обфускация может быть использована для защиты информации или для создания препятствий для анализа и понимания промпта.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "промпта"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Обфускация"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "семантику"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "смысл"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Обфускация"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "защиты информации"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "создания препятствий"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "промпта"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "Обфускация",
                "index": 0
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "промпта",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Обфускация",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "защиты информации",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Обфускация",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "создания препятствий",
                "index": 0
              }
            }
          ]
        }
      ],
      "text_797561": [
        {
          "text": "Привет, Хабр! Если вы интересуетесь NLP или просто современными DL моделями, то приглашаю вас узнать, как можно, имея всего лишь одну A100, около 30 гигабайтов текста и несколько дней обучения, решить проблему ограниченного окна контекста для русскоязычных трансформеров. А ещё сделаем несколько оптимизаций и добьёмся почти лучших метрик в бенчмарке encodechka.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Хабр"
            },
            {
              "index": 0,
              "class": "Science",
              "value": "NLP"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "DL моделями"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русскоязычных"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеров"
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "encodechka"
            }
          ],
          "relations": []
        },
        {
          "text": "Получившиеся русскоязычные модели доступны в открытом доступе на HuggingFace 🤗:",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "русскоязычные"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            },
            {
              "index": 0,
              "class": "InfoResource",
              "value": "HuggingFace"
            }
          ],
          "relations": []
        },
        {
          "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "Методы позиционного кодирования"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "расширения контекста"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "декодеров"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "англоязычных"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методов позиционного кодирования"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "энкодерах"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "DeBERTa"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "disentangled attention"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "DeBERTa",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "энкодерах",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "DeBERTa",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "англоязычных",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "disentangled attention",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "DeBERTa",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
          "terms": [
            {
              "index": 0,
              "class": "Date",
              "value": "2021"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoFormer"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Rotary Position Embedding"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Mistral"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Llama"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-NeoX"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoFormer"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русского языка"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русском"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "cointegrated/rubert-tiny2"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Дэвида Дале"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "kazzand/ru-longformer-tiny-16384"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "MTS"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "адаптация"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "увеличение контекстного окна"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "улучшение их производительности"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RoPE"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели длинного контекста"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русского языка"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "cointegrated/rubert-tiny2",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "русского языка",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "kazzand/ru-longformer-tiny-16384",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "русского языка",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Date",
                "value": "2021",
                "index": 0
              },
              "predicate": "isDateOf",
              "term2": {
                "class": "Model",
                "value": "RoFormer",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "RoFormer",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "Mistral",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "Llama",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "GPT-NeoX",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-NeoX",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Llama",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Mistral",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Lang",
                "value": "русского языка",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Model",
                "value": "RoFormer",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "cointegrated/rubert-tiny2",
                "index": 0
              },
              "predicate": "hasAuthor",
              "term2": {
                "class": "Person",
                "value": "Дэвида Дале",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "kazzand/ru-longformer-tiny-16384",
                "index": 0
              },
              "predicate": "hasAuthor",
              "term2": {
                "class": "Organization",
                "value": "MTS",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "RoPE",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Как же адаптировать модели с выученным позиционным кодированием для работы с RoPE? Очевидно, что нам нужно просто заменить одно на другое в архитектуре модели. Для этого мы можем удалить nn.Embedding слой, отвечающий за старое позиционное кодирование, а далее модифицировать SelfAttention блок в трансформере так, чтобы position_ids передавались на каждый слой и там же применялся RoPE к query и key проекциям, ведь именно так он и устроен.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "адаптировать"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформере"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "модифицировать"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RoPE"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "модифицировать",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "адаптировать",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Применив такое преобразование, мы сломаем модель почти полностью. Но это не мешает нам восстановить её способности, так как большинство весов мы не изменяли. Для этого, как простой вариант, нам потребуется просто предобучить модель в режиме RoBERTы — то есть учить MLM задачу, применив поверх модели соответствующую голову.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "RoBERTы"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "предобучить"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "MLM задачу"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            }
          ],
          "relations": []
        },
        {
          "text": "Но также, мы хотим именно восстановить способности модели, а не просто обучить её на новом датасете — это значит, дистиллировать оригинальную модель-учитель, в нашего RoPE-ученика. Мне кажется, сейчас уместнее это называть клонированием, так как размеры и архитектура остаются одинаковыми. Для того чтобы это осуществить, достаточно к MLM-лоссу добавить дистилляционный лосс, который может быть, по большому счёту, любым, кроме KL-дивергенции, так как MLM-головы никто не кладёт с весами модели, и распределения учителя на токены у нас не будет. Я решил использовать 2 лосса, по отдельности: Cosine Similarity усредненый по токенам, для каждого слоя, и InfoNCE, в симметричном режиме, только для последнего слоя, именно он используется в CLIP для сближения модальностей текста и картинок. В обоих лоссах также подбиралась и своя температура для увеличения важности близости.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модели"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "восстановить способности модели"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучить"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "оригинальную модель-учитель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoPE-ученика"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "клонированием"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "MLM-лоссу"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "дистилляционный лосс"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Cosine Similarity"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "InfoNCE"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "CLIP"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "сближения модальностей"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "текста"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "картинок"
            }
          ],
          "relations": [
           {
              "term1": {
                "class": "Method",
                "value": "клонированием",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "восстановить способности модели",
                "index": 0
              }
            },
                       {
              "term1": {
                "class": "Method",
                "value": "MLM-лоссу",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "клонированием",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "дистилляционный лосс",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "клонированием",
                "index": 0
              }
            },
                        {
              "term1": {
                "class": "Method",
                "value": "InfoNCE",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "клонированием",
                "value": "CLIP",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "InfoNCE",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "сближения модальностей",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "В качестве данных я взял ~32 гигабайта из первых 10 русскоязычных фолдов датасета CulturaX. В семплах это составило около 5 миллионов. Данный датасет хорош тем, что довольно хорошо подготовлен и отфильтрован, в русских семплах действительно мало других языков. А ещё он имеет хорошее распределение длин, тут найдутся примеры для обучения модели с практически любым контекстным окном до 16к.",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "CulturaX"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русскоязычных"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "семплах"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русских"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "семплах"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Dataset",
                "value": "CulturaX",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "русскоязычных",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Эта первая модель проекта является клоном ai-forever/ruBert-base. Для её клонирования использовался послойный cosine similarity лосс с температурой 0.5 между скрытыми представлениями ученика и учителя.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "ai-forever/ruBert-base"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "клонирования"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "послойный cosine similarity лосс"
            }
          ],
          "relations": [
                        {
              "term1": {
                "class": "Method",
                "value": "послойный cosine similarity лосс",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "клонирования",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Эта же модель является клоном hivaze/ru-e5-base, а та в свою очередь русифицированной версией intfloat/multilingual-e5-base, полученой с помощью удаления большинства лишних токенов из других языков, по методу Дэвида Дале, снижая таким образом размер словаря с 250k до 69k, что существенно облегчило её тренировку. Кстати, модель, в связи с изначальной мультиязычностью, сохраняет свойства и на английском языке, но он не являлся основным.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "hivaze/ru-e5-base"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "intfloat/multilingual-e5-base"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методу Дэвида Дале"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "тренировку"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английском языке"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "мультиязычностью"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "hivaze/ru-e5-base",
                "index": 0
              },
              "predicate": "isModificationOf",
              "term2": {
                "class": "Model",
                "value": "intfloat/multilingual-e5-base",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "методу Дэвида Дале",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "тренировку",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "мультиязычностью",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском языке",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Из-за своей особенности представления похожести текстов, e5 клонировалась с симметричным InfoNCE лоссом (как в CLIP) поверх последнего слоя и температурой 0.1. Выбор этого лосса обоснован тем, что модель должна гораздо лучше понимать разницу между разными по смыслу текстами, а не просто сближать косинусную близость со своим оригиналом. Кстати, этот же лосс использовался авторами и при обучении оригинальной e5.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "e5"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "симметричным InfoNCE лоссом"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "CLIP"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "текстами"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "косинусную близость"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучении"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "оригинальной e5"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "симметричным InfoNCE лоссом",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "e5",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "симметричным InfoNCE лоссом",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "оригинальной e5",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Я не буду подробно описывать как работает RoPE, а также его формулы и почему работают методы его интерполяции/экстраполяции. Для этого я предлагаю изучить статью Scaling Laws of RoPE-based Extrapolation или пост на хабре О методах позиционного кодирования в Transformer, а ещё оригинальную статью Roformer.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "RoPE"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методах позиционного кодирования"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Transformer"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Roformer"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "методах позиционного кодирования",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "Transformer",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Итак, использовать клонирование оригинальной модели, очевидно, можно только на длинах до 512 токенов. Но нам нужно больше, поэтому вторым этапом в пайплайне подготовки моделей стало расширение контекста — дообучение моделей только с MLM лоссом и только на текстах с длинами 512-2048 токенов.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "клонирование оригинальной модели"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "расширение контекста"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "дообучение"
            } ,
            {
              "index": 0,
              "class": "Method",
              "value": "MLM лоссом"
            }           
            ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "MLM лоссом",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "дообучение",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "В ходе экспериментов, я пришёл к выводу, что тут лучше не использовать какие-либо техники интерполяции или экстраполяции RoPE эмбедингов при обучении. Дело в том, что модель и так довольно легко сходилась на увеличенном контексте к качеству, как на 512 токенах, ещё где-то на ~200к семплах. Техники, где мы как-то влияем на эмбединги, конечно, помогают, но они нужны скорее для значительного увеличения контекста, например с 4к до 16к, так как там потребовалось бы больше времени и данных, если ничего не использовать.",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "экспериментов"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "интерполяция"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "экстраполяция"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучении"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "интерполяция",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучении",
                "index": 0
              }
            },
                        {
              "term1": {
                "class": "Method",
                "value": "экстраполяция",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучении",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Тут вы могли заметить: если мы будем дообучать модель на длинный контекст без клонирующего лосса, то как мы убедимся, что наша модель будет всё ещё похожа на оригинальную? На самом деле, очень просто — нам достаточно обучать только attention блоки, не трогая последующий MLP блок. RoPE затрагивает именно query и key, соответственно, только их проекции нам и нужно дообучать, MLP будет мапить получившиеся вектора примерно в то же место в пространстве, что и раньше, а значит характеристики вроде косинусной близости между скрытыми представлениями с оригиналом, почти не изменятся.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "дообучать"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "клонирующего лосса"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RoPE"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "дообучать"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "косинусной близости"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "query"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "key"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "MLP"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "RoPE",
                "index": 0
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "query",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "RoPE",
                "index": 0
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "key",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Для дообучения мы не используем никакие трюки со скейлингом RoPE. Но вот для инференса вполне можем, есть как минимум метод Dynamic-NTK, который позволяет применять его без дообучения. Это означает, что потенциально, модели могут использоваться и на более длинном контексте чем 2k и не очень сильно терять в качестве.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "скейлингом RoPE"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "дообучения"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Dynamic-NTK"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "инференса"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "Dynamic-NTK",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "инференса",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Ниже приведена таблица с различными оценками модели ruRoPEBert-e5-base-2k, которая была дополнительно дообучена на 512-2048 токенах, на разных длинах контекста с Dynamic-NTK скейлингом и без него.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "ruRoPEBert-e5-base-2k"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Dynamic-NTK скейлингом"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "дообучена"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "Dynamic-NTK скейлингом",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "ruRoPEBert-e5-base-2k",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Dynamic-NTK скейлингом",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "дообучена",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Как видно, скейлинг помогает адаптироваться на длины больше, чем модель видела при обучении, но тем не менее, качество всё ещё ощутимо деградирует, критичность такой деградации будет зависеть от конкретных задач. Мне кажется, для улучшения результатов, тут стоит попробовать другие мощные методы, вроде xPos или Alibi, подробнее о них можно узнать в статье A Length-Extrapolatable Transformer.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "скейлинг"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "xPos"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Alibi"
            }
          ],
          "relations": []
        },
        {
          "text": "Так как ручное вычисление внимания (eager) является абсолютно неоптимизированным, квадратичным по вычислительной сложности и слабым местом по памяти, его нельзя просто использовать как он есть, если мы хотим увеличить контекст модели в несколько раз.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "вычисление внимания"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "увеличить контекст модели"
            }
          ],
          "relations": []
        },
        {
          "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
          "terms": [
            {
              "index": 0,
              "class": "Library",
              "value": "FlashAttention"
            },
            {
              "index": 0,
              "class": "Library",
              "value": "xformers"
            },
            {
              "index": 0,
              "class": "Library",
              "value": "PyTorch"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Memory-Efficient Attention"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "FlashAttention"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Fused C++"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "нативный метод"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "torch.nn.functional.scaled_dot_product_attention"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "sdpa"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "механизм вычисления внимания"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "torch.nn.functional.scaled_dot_product_attention",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Method",
                "value": "sdpa",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "torch.nn.functional.scaled_dot_product_attention",
                "index": 0
              },
              "predicate": "isUsedIn",
              "term2": {
                "class": "Library",
                "value": "PyTorch",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Memory-Efficient Attention",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "нативный метод",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "FlashAttention",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "нативный метод",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Fused C++",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "нативный метод",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "В недавнем релизе 2.2.0 авторы PyTorch добавили поддержку FlashAttention второй версии в эту функцию, тем самым она стала, наверное, самым удобным способом добавления оптимизации вычисления внимания в модель, так как использование нативного Flash не очень удобное.",
          "terms": [
            {
              "index": 0,
              "class": "Library",
              "value": "PyTorch"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "FlashAttention"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "оптимизации вычисления внимания"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "нативного Flash"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "FlashAttention",
                "index": 0
              },
              "predicate": "isUsedIn",
              "term2": {
                "class": "Library",
                "value": "PyTorch",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "FlashAttention",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "оптимизации вычисления внимания",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "нативного Flash",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "оптимизации вычисления внимания",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "В таблице ниже я привожу бенчмарки инференса, произведённые с помощью Pytorch Profiler 12-ти слойного ruRoPEBert в конфигурации base. Замеры производились на ноутбучной RTX 3070 Ti с процессором Intel Core i7 12700H. Размер батча во всех тестах = 1.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "Pytorch Profiler"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "ruRoPEBert"
            }
          ],
          "relations": []
        },
        {
          "text": "Эффективность использования памяти при выборе sdpa вырастает в несколько раз, также модель получает ускорение относительно eager, пропорциональное увеличению размера контекста. При обучении также получается достичь ускорения в 2-3 раза и кратно сэкономить память.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "sdpa"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучении"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "sdpa",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучении",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Использовать модель, выбирая соответствующий способ вычисления внимания можно следующим образом:",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "вычисления внимания"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "вычисления внимания",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "За основу бенчмарка качества был выбран encodechka от Дэвида Дале, так как он обладает единым и простым интерфейсом запуска тестов, в нём есть немало современных моделей, а главное, что оценка производится исключительно на русскоязычных датасетах. К тому же он даёт возможность сравнить модели по их способности в семантическое представление предложений, что является важной целью данного проекта.",
          "terms": [
            {
              "index": 0,
              "class": "dataset",
              "value": "encodechka"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Дэвида Дале"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "современных моделей"
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "русскоязычных датасетах"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "семантическое представление предложений"
            }
          ],
          "relations": []
        },
        {
          "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "ruRoPEBert-e5-base-2k"
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "encodechka"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "S+W"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "sentence + word embeddings"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели e5"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "STS"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "взять температуру ниже, чем 0.1"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "использовать дополнительные датасеты"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "ruRoPEBert"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "оригинал e5-base"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "text-embedding-ada-002"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "OpenAI"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "text-embedding-ada-002",
                "index": 0
              },
              "predicate": "hasAuthor",
              "term2": {
                "class": "Organization",
                "value": "OpenAI",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "STS",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "оригинал e5-base",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "STS",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "модели e5",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "RoPEBert модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "ai-forever/ruBert-base"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "ai-forever/ruRoberta-large"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "NE1"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "NE2"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "classic модели"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "NER"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "sentence клоны e5"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "RoPEBert",
                "index": 0
              },
              "predicate": "isModificationOf",
              "term2": {
                "class": "Model",
                "value": "ai-forever/ruBert-base",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "RoPEBert",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "NER",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "ai-forever/ruBert-base",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "NER",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "ai-forever/ruRoberta-large",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "NER",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "NE1",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "classic модели",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "NE1",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "sentence клоны e5",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "NE2",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "classic модели",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "NE2",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "sentence клоны e5",
                "index": 0
              }
            }
          ]
        },
        {
          "text": "Не всё получилось, как было задумано, например, в бенчмарке STS клонированная e5 модель проигрывает оригиналу, а также модели, на мой взгляд, не очень хорошо обобщаются на длины большие, чем видели при обучении.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "клонированная e5 модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "оригиналу"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "STS"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "STS",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "клонированная e5 модель",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "клонированная e5 модель",
                "index": 0
              },
              "predicate": "isModificationOf",
              "term2": {
                "class": "Model",
                "value": "оригиналу",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Эти проблемы можно решить усовершенствовав пайплайн тренировки или некоторые гиперпараметры. Также можно заменить механизм экстраполяции, например, на xPos, Alibi или совсем недавний LongRoPE, а можно сделать спарсификацию аттеншена, как это предлагается в статье Blockwise Self-Attention for Long Document Understanding, ещё от 2019 года. Кроме того, в самой статье RoFormer упоминается, что авторы попробовали объединить RoPE с архитектурой Performer и работать посимвольно и линейно по сложности.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "e5"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "STS"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "xPos"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Alibi"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "LongRoPE"
            },
            {
              "index": 0,
              "class": "Date",
              "value": "2019"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Performer"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoFormer"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "механизм экстраполяции"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "усовершенствовав пайплайн тренировки или некоторые гиперпараметры"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "спарсификацию аттеншена"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "объединить RoPE с архитектурой Performer"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "работать посимвольно и линейно по сложности"
            }
          ],
          "relations": []
        },
        {
          "text": "Удивляет, что существует огромное количество архитектур (от старого Reformer до новейших Biderectional SSM), которые могли бы эффективно использоваться уже долгое время в энкодерах, но стандартный вариант с квадратичным вниманием — всё ещё остаётся единственным путём, которым идёт большинство SOTA моделей, особенно на русском языке. В конечном итоге этот проект делает совсем небольшое усилие в сторону современных подходов в NLP и доказывает их относительную простоту реализации.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "архитектур"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Reformer"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Biderectional SSM"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "стандартный вариант с квадратичным вниманием"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русском языке"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "большинство SOTA моделей"
            },
            {
              "index": 0,
              "class": "Science",
              "value": "NLP"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "Reformer",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "архитектур",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Biderectional SSM",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "архитектур",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "стандартный вариант с квадратичным вниманием",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "архитектур",
                "index": 0
              }
            }
            ]
        },
        {
          "text": "Веса всех моделей, их код и инструкции по запуску доступны в нашем аккаунте на HuggingFace (ссылки в начале статьи). В дальнейшем, у нас есть планы по выпуску large версий моделей и дотренировке с контекстом до 8k токенов, а также продолжение экспериментов с оптимизациями.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "HuggingFace"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "large версий"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "дотренировке"
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "экспериментов с оптимизациями"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "дотренировке",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "large версий",
                "index": 0
              }
            },
                        {
              "term1": {
                "class": "Model",
                "value": "large версий",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "моделей",
                "index": 0
              }
            }
            ]
        }
      ]
    }
  ]
}